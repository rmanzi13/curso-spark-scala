{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d23f90ef-8255-467c-8dd9-bb8f643ac69d",
   "metadata": {},
   "source": [
    "### Despliegue de Aplicaciones Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57aaf8e-2f9f-43de-9562-2b47817c22ab",
   "metadata": {},
   "source": [
    "El despliegue de aplicaciones Spark implica la ejecución de jobs en un entorno distribuido, ya sea en un clúster local (on-premise) o en la nube. Spark es una herramienta poderosa para el procesamiento de grandes volúmenes de datos, y su correcto despliegue es clave para garantizar eficiencia y escalabilidad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bea17d-a5d3-4880-ba9a-e85c3e9ab920",
   "metadata": {},
   "source": [
    "### 2. Estructura de Proyectos Spark con Scala\n",
    "Un proyecto Spark en Scala sigue una estructura organizada para facilitar el desarrollo y el despliegue:\n",
    "\n",
    "- **src/main/scala**: Contiene el código fuente de la aplicación.\n",
    "\n",
    "- **src/test/scala**: Incluye pruebas unitarias y de integración.\n",
    "\n",
    "- **build.sbt**: Archivo de configuración para SBT (Scala Build Tool).\n",
    "\n",
    "- **project**: Configuraciones adicionales de plugins y dependencias.\n",
    "\n",
    "- **resources**: Archivos de configuración, como application.conf o log4j.properties.\n",
    "\n",
    "**Ejemplo de estructura**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6921fb57-e512-4423-bbc3-aa83aee28a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mi-proyecto-spark/\r\n",
    "├── build.sbt\r\n",
    "├── src/\r\n",
    "│   ├── main/\r\n",
    "│   │   ├── scala/\r\n",
    "│   │   │   └── com/\r\n",
    "│   │   │       └── ejemplo/\r\n",
    "│   │   │           └── MiApp.scala\r\n",
    "│   │   └── resources/\r\n",
    "│   │       └── application.conf\r\n",
    "│   └── test/\r\n",
    "│       └── scala/\r\n",
    "│           └── com/\r\n",
    "│               └── ejemplo/\r\n",
    "│                   └── MiAppTest.scala\r\n",
    "└── project/\r\n",
    "    └── plugins.sbt\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506aafea-f231-456f-9963-4a14cdc4e5ec",
   "metadata": {},
   "source": [
    "### 3. Trabajando con SBT para Despliegues\n",
    "SBT (Scala Build Tool) es la herramienta estándar para compilar y empaquetar aplicaciones Spark en Scala. Algunos comandos útiles:\n",
    "\n",
    "- **Compilar**: sbt compile\n",
    "\n",
    "- **Ejecutar pruebas**: sbt test\n",
    "\n",
    "- **Empaquetar**: sbt package (genera un archivo JAR).\n",
    "\n",
    "- **Crear un JAR con dependencias**: Usar plugins como sbt-assembly.\n",
    "\n",
    "**Ejemplo de build.sbt**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a365985f-4db4-43a2-86af-bb7c33714c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "name := \"mi-proyecto-spark\"\n",
    "version := \"1.0\"\n",
    "scalaVersion := \"2.12.15\"\n",
    "libraryDependencies += \"org.apache.spark\" %% \"spark-core\" % \"3.2.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d58489-0c3d-4df1-84cc-39e0553ab2f0",
   "metadata": {},
   "source": [
    "### 4. ¿Dónde Podemos Desplegar Aplicaciones de Spark?\n",
    "Las aplicaciones Spark pueden desplegarse en diversos entornos:\n",
    "\n",
    "- **On-premise**: Clústeres locales con Hadoop YARN, Apache Mesos o Spark Standalone.\n",
    "\n",
    "- **Cloud**: Plataformas como AWS EMR, Google Dataproc, Azure HDInsight o Databricks.\n",
    "\n",
    "- **Kubernetes**: Spark también puede ejecutarse en clústeres de Kubernetes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63363518-82f5-4334-8c6c-371ae30c379e",
   "metadata": {},
   "source": [
    "### 5. Spark-Submit\n",
    "spark-submit es la herramienta principal para enviar aplicaciones Spark a un clúster. Permite especificar:\n",
    "\n",
    "- El archivo JAR de la aplicación.\n",
    "\n",
    "- Configuraciones como memoria, núcleos y modo de ejecución.\n",
    "\n",
    "- El Cluster Manager a utilizar.\n",
    "\n",
    "**Ejemplo**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7c61a7-6467-4211-9c63-525a53dc2a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark-submit \\\n",
    "  --class com.ejemplo.MiApp \\\n",
    "  --master yarn \\\n",
    "  --deploy-mode cluster \\\n",
    "  --executor-memory 4G \\\n",
    "  --num-executors 10 \\\n",
    "  mi-proyecto-spark_2.12-1.0.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60a474d-4be3-4bfb-a400-b4b420fcf670",
   "metadata": {},
   "source": [
    "**Explicación**:\n",
    "\n",
    "**1.** **spark-submit**\n",
    "Es el comando que se usa para enviar aplicaciones de Apache Spark a un clúster.\n",
    "\n",
    "**2** **--class com.ejemplo.MiApp**\n",
    "Especifica la clase principal de la aplicación (MiApp) dentro del paquete com.ejemplo.\n",
    "\n",
    "**3.** **--master yarn**\n",
    "Define el gestor de clúster que usará Spark.\n",
    "\n",
    "- yarn indica que la aplicación se ejecutará en un clúster de Hadoop con YARN como gestor de recursos.\n",
    "  \n",
    "**4.** **--deploy-mode cluster**\n",
    "Define el modo de despliegue:\n",
    "\n",
    "- cluster: El driver de la aplicación se ejecuta dentro de uno de los nodos del clúster.\n",
    "- (Alternativa: client, donde el driver se ejecuta en la máquina local desde donde se ejecuta spark-submit).\n",
    "  \n",
    "**5** **--executor-memory 4G**\n",
    "Asigna 4 GB de memoria a cada ejecutor en el clúster.\n",
    "\n",
    "**6.** **--num-executors 10**\n",
    "Especifica el número de ejecutores que se iniciarán en el clúster. Aquí se están asignando 10 ejecutores.\n",
    "\n",
    "**7.** **mi-proyecto-spark_2.12-1.0.jar**\n",
    "Es el archivo JAR que contiene la aplicación Spark.\n",
    "\n",
    "- 2.12 indica la versión de Scala utilizada.\n",
    "- 1.0 es la versión del proyecto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bef1b2-de2f-4385-808d-4c085dab900a",
   "metadata": {},
   "source": [
    "#### Resumen:\n",
    "Este comando lanza una aplicación Spark en un clúster Hadoop con YARN, ejecutándose en modo cluster, con 10 ejecutores y 4 GB de RAM por ejecutor. El código principal está en la clase com.ejemplo.MiApp, dentro del archivo mi-proyecto-spark_2.12-1.0.jar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6385c8fa-029b-4b78-8066-e6675532e76b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.12.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
