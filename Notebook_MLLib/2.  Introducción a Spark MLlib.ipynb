{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57dc41df-2cbc-45f0-8083-ffe979847872",
   "metadata": {},
   "source": [
    "## Introducción a Spark MLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48f8793-ab64-4231-9638-6603830843eb",
   "metadata": {},
   "source": [
    "### 1. Arquitectura de Spark y su ecosistema\n",
    "Apache Spark es un motor de procesamiento distribuido diseñado para manejar grandes volúmenes de datos de manera eficiente. Su arquitectura está diseñada para ser rápida, escalable y fácil de usar. A continuación, se describen sus componentes principales:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a28b06f-003b-4dbf-b21b-51714424ff43",
   "metadata": {},
   "source": [
    "#### **1.** Spark Core:\n",
    "\n",
    "- Es el núcleo de Spark y proporciona las funcionalidades básicas para la ejecución distribuida de tareas.\n",
    "\n",
    "- Incluye manejo de memoria, planificación de tareas, y manipulación de datos en formato RDD (Resilient Distributed Dataset).\n",
    "\n",
    "#### **2.** Spark SQL:\n",
    "\n",
    "  - Permite trabajar con datos estructurados y semi-estructurados utilizando SQL o la API de DataFrames.\n",
    "\n",
    "  - Facilita la integración con fuentes de datos como Hive, Parquet, JSON, etc.\n",
    "\n",
    "#### **3.** Spark Streaming:\n",
    "\n",
    "- Procesamiento de datos en tiempo real mediante micro-lotes (mini-batches).\n",
    "\n",
    "- Ideal para aplicaciones que requieren análisis en tiempo real.\n",
    "\n",
    "#### **4.** Spark MLlib:\n",
    "\n",
    "- Biblioteca de Machine Learning de Spark que proporciona algoritmos y herramientas para construir modelos escalables.\n",
    "\n",
    "- Incluye algoritmos para clasificación, regresión, clustering, filtrado colaborativo, entre otros.\n",
    "\n",
    "#### **6.** GraphX:\n",
    "\n",
    "- Biblioteca para el procesamiento de grafos y análisis de redes.\n",
    "\n",
    "#### **7.** Cluster Manager:\n",
    "\n",
    "- Spark puede ejecutarse en diferentes entornos de clúster, como Standalone, YARN, o Mesos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1eee050-bbc1-4d2c-9d6c-c643e314675c",
   "metadata": {},
   "source": [
    "### 2. Ventajas de usar Spark para Machine Learning a gran escala\n",
    "Spark MLlib es una de las bibliotecas más populares para implementar Machine Learning en entornos distribuidos. Aquí algunas de sus ventajas:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9302ee64-80f1-444b-b310-eaa461c4f649",
   "metadata": {},
   "source": [
    "#### **1.** Escalabilidad:\n",
    "\n",
    "- Spark está diseñado para manejar grandes volúmenes de datos distribuidos en un clúster, lo que lo hace ideal para aplicaciones de ML a gran escala.\n",
    "\n",
    "#### **2.** Velocidad:\n",
    "\n",
    "- Gracias a su procesamiento en memoria (in-memory processing), Spark es significativamente más rápido que otras herramientas como Hadoop MapReduce.\n",
    "\n",
    "#### **3.** Integración con el ecosistema Big Data:\n",
    "\n",
    "- Spark se integra fácilmente con otras herramientas como Hadoop, Hive, Kafka, y más, lo que facilita el flujo de trabajo en proyectos de ciencia de datos.\n",
    "\n",
    "#### **4.** MLlib:\n",
    "\n",
    "- Proporciona una amplia gama de algoritmos de ML optimizados para entornos distribuidos.\n",
    "\n",
    "- Incluye herramientas para la construcción de pipelines, evaluación de modelos y transformación de datos.\n",
    "\n",
    "#### **5.** Flexibilidad:\n",
    "\n",
    "- Spark soporta múltiples lenguajes de programación, como Scala, Python, Java y R, lo que lo hace accesible para diferentes perfiles de desarrolladores.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9db2396-357b-4cfe-beec-458eb70c8b1f",
   "metadata": {},
   "source": [
    "## 3. Componentes principales de MLlib\n",
    "Spark MLlib está diseñado para ser modular y fácil de usar. Sus componentes principales son:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73145711-53ad-4649-9b5a-9406f5c60d07",
   "metadata": {},
   "source": [
    "#### **1.** DataFrames:\n",
    "\n",
    "- Estructura de datos tabular distribuida que organiza los datos en filas y columnas.\n",
    "\n",
    "- Similar a las tablas de una base de datos relacional, pero optimizada para el procesamiento distribuido.\n",
    "\n",
    "- Permite manejar datos estructurados y semi-estructurados de manera eficiente.\n",
    "\n",
    "#### **2.** Transformers:\n",
    "\n",
    "- Son componentes que transforman un DataFrame en otro.\n",
    "\n",
    "- Ejemplos:\n",
    "\n",
    "  - **VectorAssembler**: Combina varias columnas en un solo vector de características (feature vector).\n",
    "\n",
    "  - **StringIndexer**: Convierte valores categóricos en índices numéricos.\n",
    "\n",
    "- Los transformers no aprenden de los datos, solo aplican una transformación predefinida.\n",
    "\n",
    "#### **4.** Estimators:\n",
    "\n",
    "- Son algoritmos que aprenden de los datos para generar un modelo.\n",
    "\n",
    "- Ejemplos:\n",
    "\n",
    "  - **LinearRegression**: Ajusta un modelo de regresión lineal a los datos.\n",
    "\n",
    "  - **KMeans**: Entrena un modelo de clustering utilizando el algoritmo K-Means.\n",
    "\n",
    "- Los estimadores implementan el método .fit() para entrenar un modelo.\n",
    "\n",
    "#### **5.** Pipelines:\n",
    "\n",
    "- Un pipeline es una secuencia de etapas (transformers y estimators) que se ejecutan en un orden específico.\n",
    "\n",
    "- Facilita la organización y automatización del flujo de trabajo en ML, desde la preparación de datos hasta la evaluación del modelo.\n",
    "\n",
    "- Ejemplo de un pipeline típico:\n",
    "\n",
    "  **1.** Limpieza de datos.\n",
    "\n",
    "  **2.** Transformación de características (feature engineering).\n",
    "\n",
    "  **3.** Entrenamiento del modelo.\n",
    "\n",
    "  **4.** Evaluación del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f0b19a-a372-4189-90c4-186825e60677",
   "metadata": {},
   "source": [
    "## 4. Ejemplo de uso de MLlib en Scala\n",
    "A continuación, un ejemplo básico de cómo usar MLlib en Scala para entrenar un modelo de regresión lineal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06945a7d-4278-45a5-a3ba-5c221d263840",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "// Crear una sesión de Spark\n",
    "val spark = SparkSession.builder()\n",
    "  .appName(\"MLlib Example\")\n",
    "  .master(\"local[*]\")\n",
    "  .getOrCreate()\n",
    "\n",
    "// Cargar datos\n",
    "val data = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"data.csv\")\n",
    "\n",
    "// Preparar características (features)\n",
    "val assembler = new VectorAssembler()\n",
    "  .setInputCols(Array(\"feature1\", \"feature2\"))\n",
    "  .setOutputCol(\"features\")\n",
    "\n",
    "// Definir el modelo de regresión lineal\n",
    "val lr = new LinearRegression()\n",
    "  .setLabelCol(\"label\")\n",
    "  .setFeaturesCol(\"features\")\n",
    "\n",
    "// Crear un pipeline\n",
    "val pipeline = new Pipeline()\n",
    "  .setStages(Array(assembler, lr))\n",
    "\n",
    "// Entrenar el modelo\n",
    "val model = pipeline.fit(data)\n",
    "\n",
    "// Realizar predicciones\n",
    "val predictions = model.transform(data)\n",
    "predictions.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.12.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
