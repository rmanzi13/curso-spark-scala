{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "020d40ce-f848-4320-a77f-12a194c0d7b4",
   "metadata": {},
   "source": [
    "## Tokenización y Procesamiento de Texto en Spark MLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c04cdde-94b8-4849-9f7a-1f0cef1d6389",
   "metadata": {},
   "source": [
    "El procesamiento de texto en Spark MLlib se usa para analizar y extraer información clave de textos. Uno de los primeros pasos es la tokenización, que consiste en dividir un texto en palabras individuales o \"tokens\".\n",
    "\n",
    "#### ¿Por qué es útil la tokenización?\n",
    "\n",
    "- Permite analizar palabras más usadas.\n",
    "- Facilita la limpieza del texto (eliminación de signos, palabras vacías, etc.).\n",
    "- Es un paso clave en modelos de Machine Learning con texto (Ej. NLP).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2cd16e6-40d8-478d-af7c-ba578de96207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|job_ID|         job_details|\n",
      "+------+--------------------+\n",
      "|     1|We are looking fo...|\n",
      "|     2|Join our team as ...|\n",
      "|     3|Seeking a product...|\n",
      "|     4|Looking for a bac...|\n",
      "+------+--------------------+\n",
      "\n",
      "+-------------------------------------------------------------------------------------+\n",
      "|filtered_words                                                                       |\n",
      "+-------------------------------------------------------------------------------------+\n",
      "|[looking, data, scientist, experience, python, machine, learning.]                   |\n",
      "|[join, team, software, engineer, specialized, cloud, computing, devops.]             |\n",
      "|[seeking, product, manager, strong, background, ux/ui, design, agile, methodologies.]|\n",
      "|[looking, backend, developer, expertise, java,, spring, boot,, microservices.]       |\n",
      "+-------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@1f77ca8d\n",
       "df = [job_ID: int, job_details: string]\n",
       "tokenizer = tok_0da4cd4419a0\n",
       "tokenizedDF = [job_ID: int, job_details: string ... 1 more field]\n",
       "remover = StopWordsRemover: uid=stopWords_95b14ec5e7bc, numStopWords=181, locale=en_US, caseSensitive=false\n",
       "df_filtered = [job_ID: int, job_details: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[job_ID: int, job_details: string ... 2 more fields]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.{Tokenizer, StopWordsRemover}\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "// 1)  Crear sesión de Spark\n",
    "val spark = SparkSession.builder()\n",
    "  .appName(\"Tokenizacion Job Analysis\")\n",
    "  .master(\"local[*]\") // Importante para ejecutar en local\n",
    "  .getOrCreate()\n",
    "\n",
    "// 2) argar los datos desde un CSV\n",
    "val df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"job.csv\")\n",
    "\n",
    "\n",
    "// 3) Ver las primeras filas\n",
    "df.show(5)\n",
    "\n",
    "// 4) Tokenizar el texto\n",
    "val tokenizer = new Tokenizer()\n",
    "  .setInputCol(\"job_details\")\n",
    "  .setOutputCol(\"words\")\n",
    "\n",
    "val tokenizedDF = tokenizer.transform(df)\n",
    "\n",
    "// 5) Remover palabras comunes (stopwords)\n",
    "val remover = new StopWordsRemover()\n",
    "  .setInputCol(\"words\")\n",
    "  .setOutputCol(\"filtered_words\")\n",
    "\n",
    "val df_filtered = remover.transform(tokenizedDF)\n",
    "\n",
    "// Mostrar algunas palabras filtradas\n",
    "df_filtered.select(\"filtered_words\").show(5, truncate = false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0e9840-fb1e-499f-ab8b-d3a57fcfe634",
   "metadata": {},
   "source": [
    "#### Explicación Paso a Paso\n",
    "\n",
    "**1.** Se crea una sesión de Spark usando SparkSession.builder().\n",
    "\n",
    "**2.** Se carga el archivo CSV que contiene las ofertas de empleo.\n",
    "\n",
    "**3.** Se muestran las primeras filas para verificar los datos.\n",
    "\n",
    "**4.** Se usa Tokenizer para separar las palabras en cada descripción (job_details).\n",
    "\n",
    "**5.** Se aplica StopWordsRemover para eliminar palabras comunes como \"the\", \"and\", \"in\", etc.\n",
    "\n",
    "**6.** Se muestra la columna filtered_words con las palabras clave más relevantes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f79f3a8-6b24-4324-89fa-39b3907335ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|     word|count|\n",
      "+---------+-----+\n",
      "|        a|    5|\n",
      "|       in|    4|\n",
      "|      and|    4|\n",
      "|     with|    3|\n",
      "|      for|    2|\n",
      "|  looking|    2|\n",
      "|expertise|    1|\n",
      "|  devops.|    1|\n",
      "|developer|    1|\n",
      "|    java,|    1|\n",
      "+---------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "wordsDF = [word: string, count: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[word: string, count: bigint]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// 6) Convertir las listas de palabras en filas individuales\n",
    "val wordsDF = tokenizedDF\n",
    "  .select(explode(col(\"words\")).alias(\"word\"))\n",
    "  .groupBy(\"word\")\n",
    "  .count()\n",
    "  .orderBy(desc(\"count\"))\n",
    "\n",
    "// 7) Mostrar las palabras más usadas\n",
    "wordsDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c64f70-1108-46b5-a4fa-e30bb15f47ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.12.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
