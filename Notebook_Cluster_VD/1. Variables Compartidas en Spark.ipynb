{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d024c0c-e899-4977-81e7-22ee4efff868",
   "metadata": {},
   "source": [
    "## 1. Introducción a las Variables Compartidas en Spark\n",
    "\n",
    "En Spark, cuando se ejecutan operaciones distribuidas en un clúster, a menudo es necesario compartir variables entre los nodos del clúster de manera eficiente. Spark ofrece dos tipos de variables compartidas: Broadcast Variables y Accumulators."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d10492-362f-4858-9171-c39093f5ae30",
   "metadata": {},
   "source": [
    "## 2. Broadcast Variables\n",
    "\n",
    "**Concepto**\n",
    "\n",
    "Las **Broadcast Variables** son variables que se pueden distribuir a todos los nodos del clúster de manera eficiente. En lugar de enviar una copia de la variable a cada tarea, Spark distribuye la variable una sola vez y la reutiliza en cada nodo. Esto mejora la eficiencia y reduce la sobrecarga de red.\n",
    "\n",
    "**Cuándo usarlo**:\n",
    "\n",
    "- Cuando tienes datos de solo lectura (no modificables) que deben ser accesibles en múltiples nodos.\n",
    "- Ideal para grandes colecciones de datos que no cambian durante el procesamiento.\n",
    "  \n",
    "**Cómo funciona**:\n",
    "  \n",
    "- Spark optimiza el envío de grandes conjuntos de datos a los trabajadores, almacenándolos en la memoria de cada nodo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1205b974-4891-4583-b350-118f41aa2c2b",
   "metadata": {},
   "source": [
    "### a) Usando Broadcast con RDD:\n",
    "Cuando usas RDDs en Spark, puedes aplicar la variable broadcast a las transformaciones que se realicen en el RDD, como map(), filter(), flatMap(), entre otras. La ventaja de utilizar el broadcast en este contexto es que puedes compartir grandes cantidades de datos a través de todas las particiones del RDD sin duplicarlos.\n",
    "\n",
    "**Ejemplo con RDD**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd00efa-5e8e-4b7b-9d61-27baddb80b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.broadcast.Broadcast\n",
    "\n",
    "// Crear la sesión de Spark\n",
    "val spark = SparkSession.builder().appName(\"BroadcastRDDExample\").master(\"local[*]\").getOrCreate()\n",
    "\n",
    "// Crear un mapa pequeño que se transmitirá a todos los nodos\n",
    "val productos = Map(1 -> \"Laptop\", 2 -> \"Teléfono\", 3 -> \"Tablet\")\n",
    "val productosBroadcast: Broadcast[Map[Int, String]] = spark.sparkContext.broadcast(productos)\n",
    "\n",
    "// Crear un RDD a partir de una Seq\n",
    "val data = Seq((1, 10), (2, 20), (3, 30))\n",
    "val rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "// Usar el broadcast en el RDD\n",
    "val resultado = rdd.map(x => {\n",
    "  val config = productosBroadcast.value  // Acceder a la variable broadcast\n",
    "  s\"Producto: ${config.getOrElse(x._1, \"Desconocido\")}, Cantidad: ${x._2}\"\n",
    "})\n",
    "\n",
    "// Recoger e imprimir el resultado\n",
    "resultado.collect().foreach(println)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30611cc3-5707-4367-8633-4258aae5433a",
   "metadata": {},
   "source": [
    "#### **Explicación**:\n",
    "\n",
    "**1. Iniciar SparkSession**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa454edc-1f40-4ea6-81a5-e7d000be1f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "val spark = SparkSession.builder().appName(\"BroadcastRDDExample\").master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7637cf-c14a-4a30-b0f9-ffd72b776ab1",
   "metadata": {},
   "source": [
    "- Aquí estamos creando una sesión de Spark. Es como la puerta de entrada para trabajar con Spark en Scala.\n",
    "\n",
    "- local[*] significa que vamos todos los núcleos de CPU para ejecutar el código en modo local."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd504a3-b859-4c16-834b-2ab515767228",
   "metadata": {},
   "source": [
    "**2. Definir los datos a transmitir (broadcast)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a9fb61-6600-4036-b141-d759cd3a49ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "val productos = Map(1 -> \"Laptop\", 2 -> \"Teléfono\", 3 -> \"Tablet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec83805-1f4c-4c97-a7f1-17b83f152ff9",
   "metadata": {},
   "source": [
    "- productos es un mapa con datos (en este caso, productos electrónicos) que queremos compartir con todos los nodos de Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cefa111-161c-435d-b9c4-17935d3a1f94",
   "metadata": {},
   "source": [
    "**3. Crear la variable de broadcast**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6f608a-1380-41b4-b816-a07bffefefdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "val productosBroadcast: Broadcast[Map[Int, String]] = spark.sparkContext.broadcast(productos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a93c90-6b72-478a-8b34-c44ceb7b4b77",
   "metadata": {},
   "source": [
    "- Aquí usamos broadcast para distribuir productos a todos los nodos. Ahora todos los nodos pueden acceder a esta información sin tener que enviarla repetidamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c55e36-5bfc-4b86-b0fb-a6c9b3c11771",
   "metadata": {},
   "source": [
    "**4. Crear un RDD (Resilient Distributed Dataset)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c42d1ca-1333-4cf8-a8dc-dc5a0d9234a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Crear un RDD a partir de una Seq\n",
    "val data = Seq((1, 10), (2, 20), (3, 30))\n",
    "val rdd = spark.sparkContext.parallelize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecee6f4b-17d3-4d59-a174-141a9415d77b",
   "metadata": {},
   "source": [
    "- Creamos un RDD a partir de una secuencia de números. Piensa en un RDD como una colección de datos distribuidos que Spark puede procesar en paralelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a762795-dc88-4b7e-a69d-9688548f7409",
   "metadata": {},
   "source": [
    "**5. Uso de la variable de broadcast dentro de un map**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361045fa-0827-4579-8aa0-713105b050ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Usar el broadcast en el RDD\n",
    "val resultado = rdd.map(x => {\n",
    "  val config = productosBroadcast.value  // Acceder a la variable broadcast\n",
    "  s\"Producto: ${config.getOrElse(x._1, \"Desconocido\")}, Cantidad: ${x._2}\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3974e7-71ce-4d89-a93f-27442545bb18",
   "metadata": {},
   "source": [
    "- Aquí estamos procesando cada elemento del RDD (x). Usamos productosBroadcast.value para acceder a los datos que hemos distribuido a todos los nodos.\n",
    "- Esto nos permite realizar operaciones con los datos de productos en cada parte del RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c96170c-a8d1-4081-a966-5af017c8c402",
   "metadata": {},
   "source": [
    "**6. Mostrar el resultado**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f120db6f-f272-4ae0-acd1-3c2d110c42e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Recoger e imprimir el resultado\n",
    "resultado.collect().foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d414233-c71b-433a-b7b8-144290989fbb",
   "metadata": {},
   "source": [
    "- Después de procesar los datos, los imprimimos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2eece9-e85b-4801-9a8e-ead3645e98da",
   "metadata": {},
   "outputs": [],
   "source": [
    "- Explicación de s\"Producto: ${config.getOrElse(x._1, \"Desconocido\")}, Cantidad: ${x._2}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fa28cf-6348-41d1-aa31-55c350b33a00",
   "metadata": {},
   "source": [
    "- config.getOrElse(x._1, \"Desconocido\"):\n",
    "- config: Es la variable que almacena la información de productosBroadcast\n",
    "- getOrElse(x._1, \"Desconocido\"): Este es un método de los mapas en Scala que busca un valor por su clave (x._1 en este caso)\n",
    "- x._1 representa el ID del producto. Por ejemplo, si x es (1, 10), entonces x._1 sería 1.\n",
    "- x._2 es la cantidad de productos, es decir, el segundo valor en el par. Por ejemplo, si x es (1, 10), entonces x._2 sería 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349ddfb1-0ad4-4b4f-a6cb-4f8e7288d010",
   "metadata": {},
   "source": [
    "### b) Usando Broadcast con DataFrame:\n",
    "\n",
    "En el caso de los DataFrames, es bastante común realizar transformaciones más complejas usando las funciones de Spark SQL (como select(), filter(), join(), etc.). Al igual que en los RDDs, broadcast en DataFrames es útil para evitar la duplicación de datos pequeños (como tablas de referencia) al realizar transformaciones.\n",
    "\n",
    "**Ejemplo con DataFrame**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f4ca71-f1f5-427b-8e60-72054a16d84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.broadcast.Broadcast\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "// Crear la sesión de Spark\n",
    "val spark = SparkSession.builder().appName(\"BroadcastDataFrameExample\").master(\"local[*]\").getOrCreate()\n",
    "\n",
    "// Crear un mapa pequeño que se transmitirá a todos los nodos\n",
    "val productos = Map(1 -> \"Laptop\", 2 -> \"Teléfono\", 3 -> \"Tablet\")\n",
    "val productosBroadcast: Broadcast[Map[Int, String]] = spark.sparkContext.broadcast(productos)\n",
    "\n",
    "// Crear un DataFrame de ejemplo\n",
    "val data = Seq((1, 10), (2, 20), (3, 30))\n",
    "val df = spark.createDataFrame(data).toDF(\"id_producto\", \"cantidad\")\n",
    "\n",
    "// Crear un UDF para acceder a la variable broadcast\n",
    "val getProductoNombre = udf((id: Int) => productosBroadcast.value.getOrElse(id, \"Desconocido\"))\n",
    "\n",
    "// Aplicar el UDF al DataFrame\n",
    "val resultado = df.withColumn(\"nombre_producto\", getProductoNombre($\"id_producto\"))\n",
    "resultado.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e522ed-5699-4669-bbf1-0210179d4355",
   "metadata": {},
   "source": [
    "#### Explicación:\n",
    "- En este ejemplo, creamos un DataFrame a partir de una Seq.\n",
    "- Usamos un UDF (User Defined Function) para acceder a los valores del broadcast. El UDF se aplica a una columna del DataFrame para agregar la columna nombre_producto que se obtiene a partir del mapa productosBroadcast.\n",
    "- La variable broadcast se distribuye a todas las particiones del clúster, mejorando la eficiencia al evitar duplicar el mapa en cada tarea."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f79555-1678-49ee-a7b2-1a627ad5df4d",
   "metadata": {},
   "source": [
    "### Diferencias principales entre RDD y DataFrame al usar broadcast:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a35e6e5-26b5-4888-aa13-f13ac963aa34",
   "metadata": {},
   "source": [
    "| Característica                  | RDD                                                                 | DataFrame                                                                 |\r\n",
    "|---------------------------------|---------------------------------------------------------------------|---------------------------------------------------------------------------|\r\n",
    "| **Abstracción**                 | Menos optimizado; operaciones como `map()`, `filter()`, etc.        | Más optimizado para trabajar con columnas y operaciones SQL               |\r\n",
    "| **Uso de Broadcast**            | `broadcast` se aplica directamente sobre los datos distribuidos (RDD)| `broadcast` se puede usar dentro de transformaciones como `withColumn` y funciones SQL |\r\n",
    "| **Funciones de optimización**   | Menos optimización interna                                          | Spark optimiza internamente el plan de ejecución de DataFrames (Catalizador) |\r\n",
    "| **Flexibilidad**                | Más flexible, pero menos eficiente para grandes datasets            | Más eficiente para trabajar con grandes datasets y operaciones complejas  |\r\n",
    "| **Interoperabilidad con SQL**   | No tiene integración nativa con SQL                                 | Totalmente integrado con SQL y funciones de agregación                    |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230b74ef-e76f-4d54-b2a0-e4b79b497e29",
   "metadata": {},
   "source": [
    "#### ¿Cuándo usar cada uno?\n",
    "- **RDD**: Si necesitas mayor control sobre las transformaciones y tu trabajo implica operaciones más manuales, puedes preferir trabajar con RDDs. Sin embargo, cuando trabajas con RDDs, generalmente no aprovechas al máximo las optimizaciones que ofrece Spark para DataFrames.\n",
    "\n",
    "- **DataFrame**: Si tu trabajo implica grandes conjuntos de datos con operaciones SQL o transformaciones complejas (como agregaciones, joins, etc.), lo más eficiente es usar DataFrames, ya que Spark realiza optimizaciones automáticas (como la ejecución de planes de consulta optimizados a través del Catalyst Optimizer)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c4d411-24eb-4912-b5be-8f15e4312557",
   "metadata": {},
   "source": [
    "- **RDD**: Se usa en un nivel más bajo, sin optimizaciones SQL, y es más adecuado cuando necesitas un control más directo sobre las transformaciones y acciones.\n",
    "\n",
    "- **DataFrame**: Es más adecuado para trabajar con datos estructurados y aprovechar optimizaciones y funciones SQL, haciendo el código más conciso y eficiente.\n",
    "  \n",
    "- **Broadcast**: funciona en ambos casos, pero la implementación varía ligeramente dependiendo de si estás trabajando con RDDs o DataFrames. En DataFrames, puedes usar funciones SQL y UDFs, mientras que en RDDs trabajas con transformaciones como map() o filter()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207e3092-5efd-4820-a9f3-a1eb26c9c9ee",
   "metadata": {},
   "source": [
    "###  Cuándo Broadcast NO es útil en Big Data\n",
    "#### Si los datos a compartir son muy grandes:\n",
    "\n",
    "  - Broadcast es eficiente cuando el mapa o la tabla que se comparte es pequeña (unos MB).\n",
    "    \n",
    "  - Si intentas compartir un mapa con millones de registros, puedes agotar la memoria de los ejecutores y perder rendimiento.\n",
    "    \n",
    "    \n",
    "#### Si ya puedes hacer un JOIN eficiente en Spark:\n",
    "\n",
    "  - En lugar de usar Broadcast, muchas veces es mejor hacer un JOIN con una tabla bien particionada y optimizada.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048cf4dd-e9b6-49c9-aa46-901f64f91e47",
   "metadata": {},
   "source": [
    "## 3. Accumulators\n",
    "\n",
    "**Concepto**\n",
    "\n",
    "Los Accumulators son variables que pueden ser solo agregadas, no leídas. Están diseñados para realizar sumas, conteos u otras operaciones de agregación de manera eficiente en un entorno distribuido. Los acumuladores pueden ser utilizados para hacer seguimientos de información a medida que las tareas se distribuyen.\n",
    "\n",
    "**Cuándo usarlo**:\n",
    "\n",
    "Cuando necesitas agregar datos de manera global (por ejemplo, contadores de errores, sumas acumulativas) a través de las tareas distribuidas.\n",
    "\n",
    "**Cómo funciona**:\n",
    "\n",
    "Los acumuladores permiten que varias tareas modifiquen una variable de manera segura y simultánea. Sin embargo, solo el driver puede leer su valor final. Los trabajadores pueden solo sumar o agregar a los acumuladores, pero no pueden leer su valor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd46f879-72d5-4bf5-b553-d7d112f787b9",
   "metadata": {},
   "source": [
    "### a) Usando Accumulators con RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3670d584-a662-4585-b589-17576cdab704",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "// Inicializar Spark\n",
    "val spark = SparkSession.builder().appName(\"Accumulator Example\").master(\"local[2]\").getOrCreate()\n",
    "\n",
    "// Crear un acumulador de tipo Long\n",
    "val negativeCount = spark.sparkContext.longAccumulator(\"Negative Count\")\n",
    "\n",
    "// Crear un RDD de ejemplo\n",
    "val numbers = spark.sparkContext.parallelize(Seq(1, -2, 3, -4, 5, -6, 7))\n",
    "\n",
    "// Realizar una operación y sumar en el acumulador\n",
    "numbers.foreach(x => {\n",
    "  if (x < 0) {\n",
    "    negativeCount.add(1)\n",
    "  }\n",
    "})\n",
    "\n",
    "// Mostrar el valor del acumulador (solo se puede leer en el driver)\n",
    "println(s\"Total negativos: ${negativeCount.value}\"// Detener Spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de29bd4-5a33-46c1-8eb5-712ea65e7cb1",
   "metadata": {},
   "source": [
    "#### **Explicación**:\n",
    "\n",
    "**1. Iniciar SparkSession**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32bf338-6fcf-423a-90da-6fd746de0a33",
   "metadata": {},
   "source": [
    "// Inicializar Spark\n",
    "val spark = SparkSession.builder().appName(\"Accumulator Example\").master(\"local[2]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51374aed-ce30-4727-b464-b53ebd187de7",
   "metadata": {},
   "source": [
    "**2. Crear el acumulador**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7aba51-7538-4ce3-bd22-928b59114327",
   "metadata": {},
   "outputs": [],
   "source": [
    "val negativeCount = spark.sparkContext.longAccumulator(\"Negative Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e42fbc-57fc-49b0-a5ac-ad7095817d2c",
   "metadata": {},
   "source": [
    "- Creamos un acumulador llamado negativeCount que solo podrá contar. Aquí estamos usando un acumulador de tipo Long, que solo puede sumar valores enteros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cc71f8-c027-41bc-aa98-128642b3dc88",
   "metadata": {},
   "source": [
    "**3. Crear el RDD de números**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb505e3-bb9e-4437-a20b-087dd1a02948",
   "metadata": {},
   "outputs": [],
   "source": [
    "val numbers = spark.sparkContext.parallelize(Seq(1, -2, 3, -4, 5, -6, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed301489-3eb3-4480-8f29-deab70da7edd",
   "metadata": {},
   "source": [
    "- Creamos un RDD con algunos números, incluidos algunos negativos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96278c6-7132-4dcb-bbbd-3bfca0297fa8",
   "metadata": {},
   "source": [
    "**4. Contar números negativos utilizando el acumulador**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4164263c-7983-4b40-bf96-54f4c282fe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers.foreach(x => {\n",
    "  if (x < 0) {\n",
    "    negativeCount.add(1) //add(1): En este caso, se está utilizando el método add(1) para incrementar el valor del acumulador en 1 cada vez que se \n",
    "  }                      //encuentra un número negativo.\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6902ae36-3379-41c8-8d8c-3517d12fe65a",
   "metadata": {},
   "source": [
    "- En este paso, estamos iterando a través de cada número del RDD. Si el número es negativo (x < 0), agregamos 1 al acumulador (negativeCount.add(1)).\n",
    "- El acumulador se actualiza solo, y los trabajadores no pueden leer su valor, solo pueden sumarle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af4aee1-421d-4a6e-b5e6-f43a88af0c19",
   "metadata": {},
   "source": [
    "**5. Leer el valor del acumulador**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61ff635-1a44-444f-b77f-4e49e60ab13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "println(s\"Total negativos: ${negativeCount.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3909923-26cc-4325-a08b-27997fd55428",
   "metadata": {},
   "source": [
    "- Una vez que todas las tareas están completas, leemos el valor del acumulador. Esto solo se puede hacer en el driver (el nodo que controla todo el proceso).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc87af7c-0479-4a77-be4e-274b69eda820",
   "metadata": {},
   "source": [
    "### b) Usando Accumulators con Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80a0de1-e5b5-4dcb-b7e1-42a89c5e1776",
   "metadata": {},
   "source": [
    "La diferencia clave es que con DataFrames, trabajamos con columnas y no con los elementos individuales de una secuencia. Aun así, el enfoque con el acumulador no cambia demasiado; solo que en lugar de recorrer el RDD, lo haremos con una acción foreach en un DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54726399-adc7-4577-976c-001c9b3b23c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "// Inicializar Spark\n",
    "val spark = SparkSession.builder().appName(\"Accumulator Example\").master(\"local[2]\").getOrCreate()\n",
    "\n",
    "// Crear un acumulador de tipo Long\n",
    "val negativeCount = spark.sparkContext.longAccumulator(\"Negative Count\")\n",
    "\n",
    "// Crear un DataFrame de ejemplo\n",
    "import spark.implicits._  // Para permitir la conversión de un Seq a DataFrame\n",
    "val numbersDF = Seq(1, -2, 3, -4, 5, -6, 7).toDF(\"number\")\n",
    "\n",
    "// Realizar una operación y sumar en el acumulador\n",
    "numbersDF.foreach(row => {\n",
    "  val x = row.getAs[Int](\"number\")\n",
    "  if (x < 0) {\n",
    "    negativeCount.add(1)\n",
    "  }\n",
    "})\n",
    "\n",
    "// Mostrar el valor del acumulador (solo se puede leer en el driver)\n",
    "println(s\"Total negativos: ${negativeCount.value}\")\n",
    "\n",
    "// Detener Spark\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b5bb02-e803-4ddb-b2ee-ec0fcf450b1f",
   "metadata": {},
   "source": [
    "**Explicación**\n",
    "\n",
    "**1. Inicializar Spark**: Usamos SparkSession para trabajar con DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aea35cf-2e2c-4061-a1d7-36ea1ec189dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "val spark = SparkSession.builder().appName(\"Accumulator Example\").master(\"local[2]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80119ff1-4fa5-4aff-a4b5-ed8a0db65351",
   "metadata": {},
   "source": [
    "**2. Crear un acumulador**: Creamos un acumulador de tipo Long para contar los números negativos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8773a00-9c20-4048-8859-07123fd88718",
   "metadata": {},
   "outputs": [],
   "source": [
    "val negativeCount = spark.sparkContext.longAccumulator(\"Negative Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d716fb-f317-48ef-b138-efd3097fe23e",
   "metadata": {},
   "source": [
    "**3. Crear un DataFrame de ejemplo**: En lugar de usar un RDD, creamos un DataFrame a partir de una secuencia de números."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92feb3c6-cc54-4639-8560-25d744d1acda",
   "metadata": {},
   "outputs": [],
   "source": [
    "val numbersDF = Seq(1, -2, 3, -4, 5, -6, 7).toDF(\"number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422d0b8c-a2f4-459c-954a-4791281ad0a4",
   "metadata": {},
   "source": [
    "**4. Iterar a través del DataFrame**: Usamos foreach para iterar a través de las filas del DataFrame. Accedemos a cada valor de la columna \"number\" y verificamos si es negativo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a60b27-ecb9-4b02-9b16-c2581d9d9f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "numbersDF.foreach(row => {\n",
    "  val x = row.getAs[Int](\"number\")\n",
    "  if (x < 0) {\n",
    "    negativeCount.add(1)\n",
    "  }\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bc526a-6bca-4a77-b396-9fe2e5468131",
   "metadata": {},
   "source": [
    "- row.getAs[Int](\"number\") extrae el valor de la columna \"number\" como un tipo Int para trabajar con él."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1674626f-7515-485d-9fae-ed0f18e6f1c1",
   "metadata": {},
   "source": [
    "**5. Mostrar el valor del acumulador**: Al final, mostramos el valor del acumulador que contiene la cantidad de números negativos encontrados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67baf42-9766-42cd-acf8-3220a9072854",
   "metadata": {},
   "outputs": [],
   "source": [
    "println(s\"Total negativos: ${negativeCount.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b89660-d621-4212-8943-c030e4defbd8",
   "metadata": {},
   "source": [
    "### Cuándo Accumulators NO son útiles en Big Data\n",
    "\n",
    "#### Si necesitas valores exactos:\n",
    "\n",
    "  - Los acumuladores no garantizan precisión en entornos distribuidos.\n",
    "\n",
    "  - Spark puede fallar en una tarea y volver a ejecutarla, incrementando el acumulador más de una vez.\n",
    "\n",
    "#### Si puedes usar groupBy() en su lugar:\n",
    "\n",
    "  - En lugar de un Accumulator, muchas veces un groupBy().agg() es más eficiente y preciso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300b3399-c78e-411b-bb4e-78aae5a567c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.12.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
