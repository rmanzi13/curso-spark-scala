{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "300f37a6-f2b8-4f82-b5cf-f158d5131484",
   "metadata": {},
   "source": [
    "### **1. ¿Qué es un cluster?**\n",
    "\n",
    "Conjunto de máquinas (nodos) que trabajan juntas para procesar datos de manera distribuida.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e1ff28-4eb8-410d-9943-8a997764b265",
   "metadata": {},
   "source": [
    "### 2. Arquitectura de Spark en un Clúster\n",
    "####  Componentes Clave en Spark\n",
    "\n",
    "\n",
    "**a) Driver**: Programa principal que gestiona la ejecución de tareas.\n",
    "\n",
    "\n",
    "**c) Nodo Maestro (Master Node)**: Es el nodo central del cluster. Solo existe en modo Standalone o Mesos. Si usas YARN o Kubernetes, este rol lo toma el gestor de recursos.\n",
    "\n",
    "**d) Nodos Trabajadores (Worker Nodes)**: Ejecutan los cálculos.\n",
    "\n",
    "**e) Ejecutores (Executors)**: Procesan los datos y ejecutan las tareas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70866a2e-3075-4887-8778-085adcd733b1",
   "metadata": {},
   "source": [
    "#### **a) Driver** (Dentro de Spark)\n",
    "\n",
    "- Es el programa principal que ejecuta el código Spark.\n",
    "- Contiene el SparkContext y envía tareas a los executors.\n",
    "- Aquí actúa el Spark Scheduler, que planifica los Jobs y los divide en Stages y Tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94706782-331b-48fc-bb0b-59c3877f6bde",
   "metadata": {},
   "source": [
    "#### **b) Cluster Manager** (Fuera de Spark, administra los recursos)\n",
    "\n",
    "- Es el gestor de recursos que asigna CPU, memoria y nodos.\n",
    "- Puede ser YARN, Kubernetes, Mesos o el modo Standalone de Spark.\n",
    "- El Spark Scheduler se comunica con el Cluster Manager para solicitar los recursos necesarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bf405c-70d8-4e42-a8ac-6db579ed0401",
   "metadata": {},
   "source": [
    "#### **c) Nodo Maestro** (Master Node)\n",
    "\n",
    "- Es el nodo central del cluster.\n",
    "- Solo existe en modo Standalone o Mesos.\n",
    "- Si usas YARN o Kubernetes, este rol lo toma el gestor de recursos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e90887e-8017-49fa-a0ee-be470cbdd093",
   "metadata": {},
   "source": [
    "#### **d) Nodos Trabajadores** (Worker Nodes)\n",
    "\n",
    "- Son las máquinas donde se ejecutan las tareas.\n",
    "- Dentro de cada worker hay executors que procesan los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90871489-ad8a-45ed-ac78-8098e106f40d",
   "metadata": {},
   "source": [
    "#### **e) Executors** (Dentro de los Workers)\n",
    "\n",
    "- Son los procesos que ejecutan las tareas.\n",
    "- Cada executor tiene su propia memoria y CPU asignada.\n",
    "- Devuelven los resultados al Driver cuando terminan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2baf29cd-394c-4dbc-ba0d-656e5ee28567",
   "metadata": {},
   "source": [
    "#### Punto clave: \n",
    "En local (Docker o modo standalone) el driver y los executors están en la misma máquina, pero en un cluster real están distribuidos en varias máquinas. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a17245a-5f41-47fe-907d-e215e96db36e",
   "metadata": {},
   "source": [
    "### 3. ¿Cuándo necesitamos un Clúster y qué nos aporta?\n",
    "Un clúster es necesario cuando:\n",
    "\n",
    "- Los datos son demasiado grandes para ser procesados en una sola máquina.\n",
    "\n",
    "- Se requiere **escalabilidad** para manejar cargas de trabajo crecientes.\n",
    "\n",
    "- Se necesita **paralelismo** para acelerar el procesamiento.\n",
    "\n",
    "- Se desea**tolerancia a fallos** para garantizar la continuidad en caso de errores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9384cc-b029-4ad0-9782-af306c2cbe7f",
   "metadata": {},
   "source": [
    "### 4. Modos de Ejecución de un Clúster\n",
    "Spark soporta varios modos de ejecución en clúster:\n",
    "\n",
    "- **Local**: Ejecución en una sola máquina, sin clúster.\n",
    "\n",
    "- **Standalone**: Modo nativo de Spark, donde Spark gestiona sus propios recursos.\n",
    "\n",
    "- **YARN**: Integración con Hadoop YARN para la gestión de recursos.\n",
    "\n",
    "- **Mesos**: Uso de Apache Mesos como gestor de recursos.\n",
    "\n",
    "- **Kubernetes**: Ejecución en contenedores gestionados por Kubernetes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169e7b7d-d450-43cc-a0c6-5a91ee8a7a1b",
   "metadata": {},
   "source": [
    "### 5. Configuraciones para Redes de Clústeres Escalables\n",
    "Para garantizar la escalabilidad y eficiencia de un clúster, es importante considerar:\n",
    "\n",
    "- **Particionamiento de datos**: Dividir los datos en particiones para distribuir la carga de trabajo.\n",
    "\n",
    "- **Serialización**: Usar formatos eficientes como Kryo para reducir el tamaño de los datos transmitidos.\n",
    "\n",
    "- **Uso eficiente de la memoria**: Ajustar la configuración de memoria para evitar desbordamientos y cuellos de botella."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f4f850-a109-4276-95ed-22bdfff2c776",
   "metadata": {},
   "source": [
    "### 6. Estrategias de Replicación\n",
    "La replicación es clave para garantizar la tolerancia a fallos. Consiste en almacenar copias de los datos en múltiples nodos para evitar pérdidas en caso de fallos.\n",
    "\n",
    "- **Importancia**: Asegura la disponibilidad y recuperación de datos en entornos distribuidos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304dee20-bedc-4a7a-ad8d-c087d17a966d",
   "metadata": {},
   "source": [
    "### 7. Monitorización de un Clúster\n",
    "La monitorización es esencial para optimizar el rendimiento y detectar problemas:\n",
    "\n",
    "- **UI de Spark**: Proporciona información detallada sobre el estado de las tareas, el uso de recursos y los tiempos de ejecución.\n",
    "\n",
    "- **Logs**: Registros que ayudan a identificar errores y comportamientos inesperados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7ec87c-ef14-4527-a7b2-b2ba422f3b45",
   "metadata": {},
   "source": [
    "### Ejercicio 1: Entender el Cluster y Verificar Nodos\n",
    "\n",
    "**Objetivo**: Verificar cuántos nodos tiene el cluster y cuántos ejecutores están activos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0e195e7-9bf1-43e8-b3b6-bb1659c10706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de ejecutores disponibles: 1\n",
      "Consulta Spark UI en: http://localhost:4040\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@78e35aa4\n",
       "executors = Set(04842bb6bacb:37457)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "Set(04842bb6bacb:37457)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession.builder.appName(\"ClusterCheck\").getOrCreate()\n",
    "\n",
    "// Ver número de ejecutores activos\n",
    "val executors = spark.sparkContext.getExecutorMemoryStatus.keys\n",
    "println(s\"Número de ejecutores disponibles: ${executors.size}\")\n",
    "\n",
    "// Ver detalles en localhost:4040\n",
    "println(\"Consulta Spark UI en: http://localhost:4040\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c594e4-a534-4edf-aa76-75b57d2360fe",
   "metadata": {},
   "source": [
    "#### Explicación\n",
    "\n",
    "**getExecutorMemoryStatus.keys** obtiene la lista de ejecutores activos en el cluster.\n",
    "\n",
    "**size** cuenta cuántos hay en total.\n",
    "\n",
    "**Spark UI** permite ver el estado del cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c03181-851f-44e7-bc4d-0ca03e7c7161",
   "metadata": {},
   "source": [
    "### Ejercicio 2: \n",
    "\n",
    "- Cargar y Procesar un Dataset en el Cluster\n",
    "- **Objetivo**: Leer un archivo CSV y hacer una transformación en los datos.\n",
    "\n",
    "Tenemos un archivo **ventas.csv** con el siguiente formato:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44668a80-39b6-4a71-8b88-ea106e12dd28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+------+------+\n",
      "|     fecha|  producto|cantidad|precio| total|\n",
      "+----------+----------+--------+------+------+\n",
      "|2024-01-01|    Laptop|       2|  1200|2400.0|\n",
      "|2024-01-02|Smartphone|       5|   800|4000.0|\n",
      "|2024-01-03|    Tablet|       3|   600|1800.0|\n",
      "+----------+----------+--------+------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lastException = null\n",
       "spark = org.apache.spark.sql.SparkSession@78e35aa4\n",
       "df = [fecha: string, producto: string ... 2 more fields]\n",
       "ventasTotales = [fecha: string, producto: string ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[fecha: string, producto: string ... 3 more fields]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession.builder.appName(\"ProcesarVentas\").getOrCreate()\n",
    "val df = spark.read.option(\"header\", \"true\").csv(\"ventas.csv\")\n",
    "\n",
    "// Calcular ventas totales por producto\n",
    "val ventasTotales = df.withColumn(\"total\", col(\"cantidad\") * col(\"precio\"))\n",
    "ventasTotales.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5894b20a-52d5-4d92-866e-2599f35204ce",
   "metadata": {},
   "source": [
    "#### Explicación\n",
    "- read.csv() carga el archivo con encabezado.\n",
    "- withColumn(\"total\", col(\"cantidad\") * col(\"precio\")) crea una nueva columna con el total.\n",
    "- .show() muestra los datos en consola."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35855bf-fb2b-4db1-a390-40b5e19e3017",
   "metadata": {},
   "source": [
    "### Ejercicio 3: Contar Palabras en un Cluster\n",
    "\n",
    "**Objetivo**: Implementar el clásico Word Count usando RDDs en Spark.\n",
    "\n",
    "Tenemos un **archivo txt** y ejecutams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0206973c-3aeb-4c13-8c20-7e28e90e6351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Hola,2)\n",
      "(Scala,1)\n",
      "(Cluster,2)\n",
      "(Spark,3)\n",
      "(Datos,1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "textFile = texto.txt MapPartitionsRDD[44] at textFile at <console>:38\n",
       "wordCounts = ShuffledRDD[47] at reduceByKey at <console>:43\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "ShuffledRDD[47] at reduceByKey at <console>:43"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val textFile = spark.sparkContext.textFile(\"texto.txt\")\n",
    "\n",
    "val wordCounts = textFile\n",
    "  .flatMap(line => line.split(\" \"))\n",
    "  .map(word => (word, 1))\n",
    "  .reduceByKey(_ + _)\n",
    "\n",
    "wordCounts.collect().foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0725f7f3-acca-4629-a6f5-21303f8e373a",
   "metadata": {},
   "source": [
    "#### Explicación\n",
    "\n",
    "**1.** textFile carga el archivo en un RDD.\n",
    "\n",
    "**2.** flatMap divide el texto en palabras.\n",
    "\n",
    "**3.** map convierte cada palabra en un par (palabra, 1).\n",
    "\n",
    "**4.** reduceByKey(_ + _) suma los valores por cada palabra.\n",
    "\n",
    "**5.** collect().foreach(println) imprime los resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02847810-2e56-41c7-be63-fd20146c0877",
   "metadata": {},
   "source": [
    "### Ejercicio 4: \n",
    "- Optimizar una Tarea en el Cluster\n",
    "- **Objetivo**: Mostrar la diferencia entre lazy evaluation y persistencia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663901e7-fe73-43ba-9d7b-bf61576466e9",
   "metadata": {},
   "source": [
    "#### Vamos a ejecutar el siguente código dos veces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16bc692c-b53f-4800-8b70-5eeb8cbcdabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "333333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rdd = ParallelCollectionRDD[70] at parallelize at <console>:43\n",
       "result = MapPartitionsRDD[72] at filter at <console>:46\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[72] at filter at <console>:46"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = spark.sparkContext.parallelize(1 to 1000000)\n",
    "\n",
    "// Sin persistencia\n",
    "val result = rdd.map(x => x * 2).filter(_ % 3 == 0)\n",
    "println(result.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc658c46-ffeb-4826-94af-072794a80a42",
   "metadata": {},
   "source": [
    "#### Luego, probar con persistencia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ba1b7d2-7647-4cd3-b62b-d5f079d33082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "333333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rdd2 = ParallelCollectionRDD[73] at parallelize at <console>:45\n",
       "result2 = MapPartitionsRDD[75] at filter at <console>:47\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[75] at filter at <console>:47"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.storage.StorageLevel\n",
    "\n",
    "val rdd2 = spark.sparkContext.parallelize(1 to 1000000).persist(StorageLevel.MEMORY_ONLY)\n",
    "\n",
    "val result2 = rdd2.map(x => x * 2).filter(_ % 3 == 0)\n",
    "println(result2.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863359d4-7c84-4d4e-86b3-384544b87685",
   "metadata": {},
   "source": [
    "#### Explicación\n",
    "\n",
    "- En la primera versión, Spark recalcula todo cada vez.\n",
    "- Con .persist(), los datos quedan en memoria y la segunda ejecución es más rápida.\n",
    "- Se pueden ver las diferencias en localhost:4040."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ec90c3-46ba-4a00-aa8c-7cc3ef6e7343",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.12.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
