{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05474fcd-c4e0-439f-8c77-08894b9ef019",
   "metadata": {},
   "source": [
    "## Big Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eca497c-a525-4fd1-9fa5-da38e75826b7",
   "metadata": {},
   "source": [
    "Big Data se refiere a las técnicas y herramientas utilizadas para procesar grandes volúmenes de datos que no pueden ser manejados por una sola computadora debido a su volumen, velocidad, variedad y veracidad (las 4 \"V\").\n",
    "\n",
    "- **Volumen:** Cantidad masiva de datos.\n",
    "\n",
    "- **Velocidad:** Rapidez con la que se generan y procesan los datos.\n",
    "\n",
    "- **Variedad:** Diferentes tipos de datos (estructurados, semi-estructurados, no estructurados).\n",
    "\n",
    "- **Veracidad:** Calidad y confiabilidad de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2532c1-f149-400c-851a-c1f7a1f038bd",
   "metadata": {},
   "source": [
    "## Ecosistema de Apache Hadoop\n",
    "\n",
    "Apache Hadoop es un framework de código abierto diseñado para el procesamiento distribuido de grandes volúmenes de datos en clústeres de computadoras. Hadoop se compone de varios módulos, entre los que destacan:\n",
    "\n",
    "- **HDFS (Hadoop Distributed File System):** Sistema de archivos distribuido que almacena datos en múltiples nodos.\n",
    "\n",
    "- **MapReduce:** Modelo de programación para procesar grandes conjuntos de datos en paralelo.\n",
    "\n",
    "- **YARN (Yet Another Resource Negotiator):** Gestor de recursos que administra los recursos del clúster y planifica las tareas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ffc78b-5fdc-4877-b1bc-963a397ed2a8",
   "metadata": {},
   "source": [
    "<img src=\"images\\Ecosistena_Apache_Hadoop.png\" alt=\"Ecosistema Apache Hadoop\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247474b2-413e-4451-89c1-8eaf6e370cf0",
   "metadata": {},
   "source": [
    "#### 4. **Apache Hive**\n",
    "\n",
    "Apache Hive es una herramienta construida sobre Hadoop que permite realizar consultas SQL sobre grandes volúmenes de datos almacenados en HDFS. Hive convierte las consultas SQL en trabajos de MapReduce, lo que facilita el análisis de datos para usuarios familiarizados con SQL.\n",
    "\n",
    "**Integración de Hive con Spark**\n",
    "\n",
    "Spark puede leer y escribir datos en tablas de Hive, lo que permite combinar la potencia de Spark con la facilidad de uso de Hive para consultas SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861686de-651b-4f16-a796-041d0079047e",
   "metadata": {},
   "source": [
    "### Data Lakehouse\n",
    "Un Data Lakehouse es un repositorio centralizado que almacena grandes cantidades de datos en su formato crudo, tanto estructurados como no estructurados. Combina las ventajas de un Data Lake (almacenamiento de datos en bruto) con las de un Data Warehouse (gestión de datos estructurados)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16ae7ba-71b7-4930-a95d-96f27ee2bfd1",
   "metadata": {},
   "source": [
    "## RDD (Resilient Distributed Datasets) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722d9180-6b5d-4c21-9be4-40c8a3ebf8bd",
   "metadata": {},
   "source": [
    "### **a) Fundamentos de Spark**  \n",
    "\n",
    "- Los **RDD** (Resilient Distributed Datasets) son la estructura de datos fundamental en Spark. Representan una colección distribuida de elementos que pueden procesarse en paralelo.  \n",
    "- **Spark SQL** y **DataFrames** están construidos sobre los RDD, pero abstraen gran parte de la complejidad para facilitar el trabajo con datos estructurados.  \n",
    "\n",
    "---\n",
    "\n",
    "### **b) Entendimiento de la arquitectura**  \n",
    "\n",
    "Los **RDD** son la base del procesamiento distribuido en Spark. Son colecciones distribuidas y tolerantes a fallos que se procesan en paralelo en un clúster. Su arquitectura se basa en los siguientes principios:  \n",
    "\n",
    "#### **1. Distribuido**  \n",
    "- Los datos se dividen en **particiones** y se distribuyen en los nodos del clúster.  \n",
    "- Cada partición se procesa en paralelo en diferentes nodos, optimizando el rendimiento.  \n",
    "\n",
    "#### **2. Resiliente (Tolerante a Fallos)**  \n",
    "- Los RDD registran su **linaje** (historial de transformaciones) para reconstruir datos en caso de fallos.  \n",
    "- Si un nodo falla, Spark puede **recrear las particiones perdidas** utilizando el linaje.  \n",
    "\n",
    "#### **3. Inmutable**  \n",
    "- Los RDD **no pueden modificarse**. Cada transformación genera un nuevo RDD en lugar de alterar el original.  \n",
    "- Ejemplos de transformaciones: `map`, `filter`, `reduce`, que producen nuevos RDD.  \n",
    "\n",
    "#### **4. Lazy Evaluation (Evaluación Perezosa)**  \n",
    "- Las transformaciones en RDD **no se ejecutan inmediatamente**.  \n",
    "- Spark espera hasta que se requiere una acción (como `collect` o `count`) para ejecutar las operaciones, optimizando así el procesamiento.  \n",
    "\n",
    "---\n",
    "\n",
    "### **c) Transición natural**  \n",
    "Los **RDD** ofrecen flexibilidad, pero pueden ser menos eficientes para operaciones estructuradas.  \n",
    "- La transición de **RDD a DataFrames** permite que Spark optimice el procesamiento de datos estructurados mediante un motor de consultas más eficiente.  \n",
    "\n",
    "---\n",
    "\n",
    "### **d) Contexto histórico**  \n",
    "- Spark se diseñó inicialmente con **RDD** como su estructura central.  \n",
    "- Con el tiempo, se introdujeron **DataFrames y Spark SQL** para simplificar la manipulación de datos estructurados y mejorar el rendimiento.  \n",
    "- Entender esta evolución es clave para aprovechar al máximo Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73252f7d-d16d-4a2b-b245-0efdaaf9c174",
   "metadata": {},
   "source": [
    "**Ejemplo de RDD en Scala:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056cecc0-4d4e-4ef5-8166-d5add357bb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "// Iniciar una SparkSession\n",
    "val spark = SparkSession.builder()\n",
    "  .appName(\"Introducción a RDD\")\n",
    "  .master(\"local[*]\")\n",
    "  .getOrCreate()\n",
    "\n",
    "// Crear un RDD a partir de una lista\n",
    "val datos: RDD[Int] = spark.sparkContext.parallelize(Seq(1, 2, 3, 4, 5))\n",
    "\n",
    "// Transformación: Multiplicar cada elemento por 2\n",
    "val rddTransformado: RDD[Int] = datos.map(x => x * 2)\n",
    "\n",
    "// Acción: Recopilar los resultados\n",
    "val resultados: Array[Int] = rddTransformado.collect()\n",
    "\n",
    "// Mostrar resultados\n",
    "resultados.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8059a13c-82f9-47fe-adc6-8d0a83665e43",
   "metadata": {},
   "source": [
    "**Ejemplo de transición a DataFrames:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6eb8dd8-117f-41b9-9984-e4ade9b51dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Crear un DataFrame a partir de un RDD\n",
    "val df = spark.createDataFrame(rddTransformado.map(x => (x, x * 2)), Seq(\"col1\", \"col2\"))\n",
    "\n",
    "// Mostrar el DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7ddab3-3467-47ea-af9a-6fbec2d9aa77",
   "metadata": {},
   "source": [
    "**Explicación:**\n",
    "- **Paso 1: Partimos de un RDD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1832af4-3407-4a51-a21f-3082480a6a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "rddTransformado.map(x => (x, x * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d216c2-ebdb-4915-8eca-4ed74c1842a5",
   "metadata": {},
   "source": [
    "- Aquí, rddTransformado es un RDD que contiene datos numéricos.\n",
    "- Se aplica map(x => (x, x * 2)) para transformar cada elemento x en una tupla (x, x * 2), agregando una nueva columna que duplica el valor original."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c116fd48-b612-4a53-b611-399109f2201c",
   "metadata": {},
   "source": [
    "**Ejemplo de cómo quedaría el RDD después de esta transformación:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c635575e-4e76-4d80-a51f-112310fee624",
   "metadata": {},
   "outputs": [],
   "source": [
    "[(1,2), (2,4), (3,6), (4,8)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7936bd20-f8a0-4757-8b0b-d37c621a2d18",
   "metadata": {},
   "source": [
    "- **Paso 2: Convertimos el RDD en un DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de1bc35-712e-4bc9-b391-a6242dbdf5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "val df = spark.createDataFrame(rddTransformado.map(x => (x, x * 2)), Seq(\"col1\", \"col2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4729e04-627e-49a1-aff3-33724e6a7522",
   "metadata": {},
   "source": [
    "- Se usa spark.createDataFrame(...) para convertir el RDD en un DataFrame.\n",
    "- La función recibe el RDD transformado y una lista con los nombres de las columnas: \"col1\" y \"col2\".\n",
    "- El DataFrame resultante tiene una estructura tabular, más parecida a una tabla SQL o un archivo CSV."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b35952-cdb7-4f2b-ada4-8689e0c9244f",
   "metadata": {},
   "source": [
    "- **Paso 3: Visualizamos el DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8506636f-aeab-47ff-8f6a-93f4dcae4dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe7060d-a17f-4b17-b158-4114456758eb",
   "metadata": {},
   "source": [
    "+----+----+\n",
    "|col1|col2|\n",
    "+----+----+\n",
    "|   1|   2|\n",
    "|   2|   4|\n",
    "|   3|   6|\n",
    "|   4|   8|\n",
    "+----+----+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d73deb-6669-41cf-af92-a7ac06c302ff",
   "metadata": {},
   "source": [
    "**Ejemplo de comparación RDD vs DataFrame:**\n",
    "- **Filtrado en RDD:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971c8ff7-8f8c-44b5-94db-48429d859ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "val rdd = spark.sparkContext.parallelize(Seq(1, 2, 3, 4, 5))\n",
    "val rddFiltrado = rdd.filter(x => x > 3)\n",
    "rddFiltrado.collect().foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ed0a80-3667-409e-ba6a-fa83d39a7390",
   "metadata": {},
   "source": [
    "- **Filtrado en DataFrame:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c1c373-8386-4f4a-ade2-54c2cc834fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "val df = spark.createDataFrame(Seq((1), (2), (3), (4), (5))).toDF(\"valor\")\n",
    "val dfFiltrado = df.filter(\"valor > 3\")\n",
    "dfFiltrado.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193bbe01-e625-488c-bb69-e28c78fe6d27",
   "metadata": {},
   "source": [
    "## **Apache Spark**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530619c9-efe0-477a-b468-7efe45810d25",
   "metadata": {},
   "source": [
    "#### **1. ¿Qué es Apache Spark?**\n",
    "\n",
    "Apache Spark es un motor de procesamiento de datos distribuido y en memoria, diseñado para ser rápido y eficiente en el manejo de grandes volúmenes de datos. A diferencia de Hadoop MapReduce, que escribe y lee datos del disco en cada paso, Spark mantiene los datos en memoria, lo que lo hace hasta 100 veces más rápido para ciertas cargas de trabajo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5dbf69-8bd4-40be-8fb0-f1f312e5e712",
   "metadata": {},
   "source": [
    "#### **Ventajas de Spark sobre Hadoop MapReduce:**\n",
    "\n",
    "- **Velocidad:** Al procesar datos en memoria, Spark evita la sobrecarga de E/S (entrada/salida) en disco, lo que lo hace ideal para aplicaciones que requieren baja latencia.\n",
    "\n",
    "- **Facilidad de uso:** Spark ofrece APIs en Scala, Java, Python y R, lo que lo hace accesible para una amplia gama de desarrolladores.\n",
    "\n",
    "- **Unificación:** Spark integra múltiples componentes (SQL, streaming, machine learning, procesamiento de grafos) en una sola plataforma, eliminando la necesidad de usar herramientas separadas.\n",
    "\n",
    "- **Tolerancia a fallos:** Spark utiliza RDD (Resilient Distributed Datasets), que son colecciones de datos distribuidos y tolerantes a fallos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbe30b1-fbdd-4028-8c77-ade83f4d7fca",
   "metadata": {},
   "source": [
    "#### **Casos de uso en ciencia de datos:**\n",
    "\n",
    "- **Procesamiento de datos a gran escala:** Limpieza, transformación y análisis de grandes volúmenes de datos.\n",
    "\n",
    "- **Machine Learning:** Entrenamiento de modelos con grandes conjuntos de datos usando MLlib.\n",
    "\n",
    "- **Streaming en tiempo real:** Procesamiento de flujos de datos en tiempo real con Spark Streaming.\n",
    "\n",
    "- **Análisis de grafos:** Procesamiento de datos en forma de grafos con GraphX."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fe1ca4-a656-4c9b-be55-97173c224c34",
   "metadata": {},
   "source": [
    "#### **2. Componentes principales de Spark**\n",
    "\n",
    "Spark es una plataforma modular que incluye varios componentes para diferentes tipos de procesamiento de datos:\n",
    "\n",
    "**1. Spark Core:**\n",
    "\n",
    "- Es el núcleo de Spark y proporciona la funcionalidad básica para la ejecución distribuida de tareas.\n",
    "\n",
    "- Incluye soporte para RDD, que son la base de todas las operaciones en Spark.\n",
    "\n",
    "**2. Spark SQL:**\n",
    "\n",
    "- Permite trabajar con datos estructurados usando consultas SQL.\n",
    "\n",
    "- Proporciona una interfaz unificada para trabajar con DataFrames y Datasets.\n",
    "\n",
    "- Ejemplo: Consultas SQL sobre grandes volúmenes de datos almacenados en HDFS o bases de datos.\n",
    "\n",
    "**3. Spark Streaming:**\n",
    "\n",
    "- Permite procesar flujos de datos en tiempo real.\n",
    "\n",
    "- Ejemplo: Análisis de logs en tiempo real o procesamiento de datos de sensores.\n",
    "\n",
    "**4. MLlib (Machine Learning Library):**\n",
    "\n",
    "- Biblioteca de machine learning escalable que incluye algoritmos como clasificación, regresión, clustering y recomendación.\n",
    "\n",
    "- Ejemplo: Entrenamiento de modelos predictivos con grandes conjuntos de datos.\n",
    "\n",
    "**5. GraphX:**\n",
    "\n",
    "- Biblioteca para el procesamiento de grafos y análisis de redes.\n",
    "\n",
    "- Ejemplo: Análisis de redes sociales o detección de comunidades en grafos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d64b8c9-3970-4e4f-95b8-06f919c8623a",
   "metadata": {},
   "source": [
    "#### **3. SparkSession: Punto de entrada a la funcionalidad de Spark**\n",
    "\n",
    "SparkSession es la interfaz principal para interactuar con Spark. Es el punto de entrada para crear DataFrames, ejecutar consultas SQL y acceder a otras funcionalidades de Spark.\n",
    "\n",
    "**¿Qué hace SparkSession?**\n",
    "\n",
    "- Gestiona la conexión con el clúster de Spark.\n",
    "\n",
    "- Proporciona acceso a las APIs de Spark (DataFrames, SQL, streaming, etc.).\n",
    "\n",
    "- Configura las propiedades de la aplicación (por ejemplo, el nombre de la aplicación, el modo de ejecución, etc.).\n",
    "\n",
    "**Cómo crear una SparkSession en Scala:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbf306b-ceb9-471a-bf09-0636523d03f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "// Crear una SparkSession\n",
    "val spark = SparkSession.builder()\n",
    "  .appName(\"MiPrimeraAplicacionSpark\")  // Nombre de la aplicación\n",
    "  .master(\"local[*]\")                   // Ejecutar en local con todos los núcleos\n",
    "  .getOrCreate()\n",
    "\n",
    "// Ejemplo: Crear un DataFrame a partir de una lista\n",
    "val datos = Seq((\"Alice\", 34), (\"Bob\", 45), (\"Cathy\", 29))\n",
    "val df = spark.createDataFrame(datos).toDF(\"Nombre\", \"Edad\")\n",
    "\n",
    "// Mostrar el DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8b1d1a-8fc7-4a31-b510-a312b3633285",
   "metadata": {},
   "source": [
    "**Explicación del código:**  \n",
    "\n",
    "\r",
    "- **appName(\"MiPrimeraAplicacionSpark\")**: Asigna un nombre a la aplicación, que aparecerá en la interfaz web de Spark.\r\n",
    "\r\n",
    "- **master(\"local[*]\")**: Especifica que la aplicación se ejecutará en modo local, utilizando todos los núcleos disponibles.\r\n",
    "\r\n",
    "- **.getOrCreate()**: Crea una nueva SparkSession o reutiliza una existente.\r\n",
    "ete.\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dbcb2e-e7df-47b6-a5c4-0c54f2723179",
   "metadata": {},
   "source": [
    "**El Shell de Spark en Scala**\n",
    "\n",
    "- La forma más sencilla de interactuar con Spark es a través del shell de Scala, que se inicia con el comando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5052d438-e60a-4ec3-a9f2-0e6baa7e72fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark-shell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f67e71-f90a-40c1-a612-0dc6b1da157f",
   "metadata": {},
   "source": [
    "- Este shell inicia una SparkSession automáticamente, permitiendo ejecutar código Scala directamente en un entorno interactivo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.12.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
