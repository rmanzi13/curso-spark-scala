{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b8afd9b-c866-4b9f-b3a6-0867993a7800",
   "metadata": {},
   "source": [
    "## 1. DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04050ab-0602-4467-aa39-84683db5ed0e",
   "metadata": {},
   "source": [
    "### ¿Qué es un DataFrame?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f4f511-bcce-41c4-86d1-6f3350e5c4a8",
   "metadata": {},
   "source": [
    "Un DataFrame es una estructura de datos tabular con un esquema definido, similar a una tabla en una base de datos relacional. Está optimizado para el procesamiento distribuido y es una de las abstracciones más utilizadas en Spark para trabajar con datos estructurados, pero no tiene un tipo fuerte en las filas (por ejemplo, las filas son objetos genéricos de tipo Row)\n",
    "\n",
    "**Características principales:**  \n",
    "\n",
    "- **Estructura tabular:** Organiza los datos en filas y columnas.\n",
    "  \n",
    "\n",
    "- **Esquema:** Define el tipo de datos de cada columna (por ejemplo, entero, cadena, fecha).\n",
    "  \n",
    "\n",
    "- **Optimización:** Utiliza el optimizador Catalyst y el motor de ejecución Tungsten para mejorar el rendimiento.\n",
    "  \n",
    "\n",
    "- **Interoperabilidad:** Puede crearse a partir de múltiples fuentes de datos (CSV, JSON, Parquet, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655bcdd8-a5bd-4361-b460-eb539085e6a3",
   "metadata": {},
   "source": [
    "**Similitudes con tablas SQL:**  \n",
    "\n",
    "- Los DataFrames son conceptualmente equivalentes a las tablas en SQL.\n",
    "\n",
    "- Permiten ejecutar consultas SQL directamente sobre ellos.\n",
    "\n",
    "- Ejemplo: SELECT * FROM df WHERE columna > 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b0976c-565b-4016-80c0-965dbd87c344",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60a3ea55-50a1-4b9d-b748-5c912b8a110a",
   "metadata": {},
   "source": [
    "#### **Creación de DataFrames:**\n",
    "\n",
    "Los DataFrames pueden crearse a partir de diversas fuentes de datos:\n",
    "\n",
    "**1. Desde un archivo CSV:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c0288e-563f-4d79-a3b8-4b299f277087",
   "metadata": {},
   "outputs": [],
   "source": [
    "val df = spark.read\n",
    "  .option(\"header\", \"true\")\n",
    "  .csv(\"ruta/al/archivo.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b69c34-7c23-4b0c-aae6-a3819724b442",
   "metadata": {},
   "source": [
    "**2. Desde un archivo JSON:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca3a218-1465-4aa3-bc95-5f282ba45140",
   "metadata": {},
   "outputs": [],
   "source": [
    "val df = spark.read.json(\"ruta/al/archivo.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "477406be-04d2-45c9-a1bc-586c78b86e0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@2bf7ca64\n",
       "df = [Bebida_descripcion: string, Bebida_img: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[Bebida_descripcion: string, Bebida_img: string ... 2 more fields]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession.builder().appName(\"CargaJSON\").getOrCreate()\n",
    "\n",
    "val df = spark.read.json(\"datos_bebidas_conserje.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1328e49-51f1-43df-9a73-af404806f1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Bebida_descripcion: string (nullable = true)\n",
      " |-- Bebida_img: string (nullable = true)\n",
      " |-- Bebida_name: string (nullable = true)\n",
      " |-- _Id: long (nullable = true)\n",
      "\n",
      "+--------------------+------------------+------------+---+\n",
      "|  Bebida_descripcion|        Bebida_img| Bebida_name|_Id|\n",
      "+--------------------+------------------+------------+---+\n",
      "|Bebida tradiciona...|[URL de la imagen]|     Sangría|201|\n",
      "|Agua mineral natu...|[URL de la imagen]|Agua Mineral|202|\n",
      "|       Cerveza local|[URL de la imagen]|     Cerveza|203|\n",
      "|Vino tinto de la ...|[URL de la imagen]|  Vino Tinto|204|\n",
      "|Té verde aromátic...|[URL de la imagen]|    Té Verde|205|\n",
      "+--------------------+------------------+------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f16f6dc-d63d-4727-ba9e-ed80aaa9ee98",
   "metadata": {},
   "source": [
    "**3. Desde un archivo Parquet:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658642f1-fcd3-49c3-b9d3-4e7dcfb2705b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val df = spark.read.parquet(\"ruta/al/archivo.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d007757-7597-48f5-9f04-6262c873a049",
   "metadata": {},
   "source": [
    "**4. Desde una colección de Scala:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d6bbea8-1318-4f39-8d7f-d50603f77727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datos = List((Alice,34), (Bob,45), (Cathy,29))\n",
       "df = [Nombre: string, Edad: int]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[Nombre: string, Edad: int]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val datos = Seq((\"Alice\", 34), (\"Bob\", 45), (\"Cathy\", 29))\n",
    "val df = spark.createDataFrame(datos).toDF(\"Nombre\", \"Edad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c23ae8-229b-4689-87b2-b935274cb2df",
   "metadata": {},
   "source": [
    "#### **Operaciones comunes con DataFrames:**\n",
    "\n",
    "- **Filtrado:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d52c3338-6526-4280-a977-4745b21a28ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfFiltrado = [Nombre: string, Edad: int]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[Nombre: string, Edad: int]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfFiltrado = df.filter(\"Edad > 30\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2db6b2-b67c-4cc4-8499-401fdad0b2ce",
   "metadata": {},
   "source": [
    "- **Selección de columnas:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d338a7-bd81-4a20-aa92-ab690244c8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "val nombres = df.select(\"Nombre\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3933431c-d9c0-4c2b-9127-13e940b7e77a",
   "metadata": {},
   "source": [
    "- **Agregaciones:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b2e0e3-00f0-4591-b554-99a5e1b75b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val promedioEdad = df.agg(avg(\"Edad\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416bc91b-de5c-40b9-b84d-d0441b8a5b65",
   "metadata": {},
   "source": [
    "## 2. Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9185b92-7e68-459b-b74e-86ce05d2ccb2",
   "metadata": {},
   "source": [
    "### ¿Qué es un Dataset?\n",
    "\n",
    "Un Dataset es una extensión de los DataFrames que proporciona tipado estático en tiempo de compilación. Combina las ventajas de los DataFrames (optimización y esquema) con la seguridad de tipos de los RDD. Tienen un tipo de datos fuerte (es decir, tiene un tipo definido por una case class o tipo específico)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4475f01d-d402-4294-9a90-e5ec6aa6a101",
   "metadata": {},
   "source": [
    "#### Ventajas de los Datasets:\n",
    "\n",
    "- **Tipado estático**: Permite detectar errores en tiempo de compilación.  \n",
    "\n",
    "- **Interoperabilidad**: Puede trabajar con objetos de Scala/Java de manera segura.  \n",
    "\n",
    "- **Optimización**: Al igual que los DataFrames, utiliza el optimizador Catalyst.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4cb7f1-4b58-413e-8a24-50e94e6d7e9d",
   "metadata": {},
   "source": [
    "#### Creación de Dataset\r\n",
    "\r\n",
    "Los Datasets se crean a partir de DataFrames o colecciones de objetos.  \r\n",
    "\r\n",
    "**1. Desde un DataFrame:**\r\n",
    "e:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ff4423-0d28-46a9-8dfc-2a0549ff8e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "case class Persona(Nombre: String, Edad: Int)\n",
    "val ds = df.as[Persona]  // Convertir DataFrame a Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae149e41-34e9-4ccc-a8b2-26c84dd2a6e1",
   "metadata": {},
   "source": [
    "#### El método .as:\n",
    "- se utiliza para convertir un DataFrame a un Dataset en Spark.\n",
    "\n",
    "- Esto es posible porque Dataset en Spark es una abstracción que combina las características de un DataFrame (estructurado, con esquema) con las de un RDD (tipado y fuerte control de tipos)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d5b769-c85b-456c-aa8f-4b9e57a04795",
   "metadata": {},
   "source": [
    "**2. Desde una colección de Scala:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05c9010-b2b7-4b54-a1bd-4d40034b1145",
   "metadata": {},
   "outputs": [],
   "source": [
    "val datos = Seq(Persona(\"Alice\", 34), Persona(\"Bob\", 45))\n",
    "val ds = spark.createDataset(datos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a4c51b-d836-45d8-8e2e-54d047ed6f93",
   "metadata": {},
   "source": [
    "#### Operaciones comunes con Datasets:\n",
    "- **Filtrado**o:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8ad043-76f7-42a1-93c9-75a212204bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "val dsFiltrado = ds.filter(_.Edad > 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f753d88-6698-4880-a575-06d891158db9",
   "metadata": {},
   "source": [
    "- **Transformaciones**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b4c982-5102-4c0f-a4a8-fcbf813533b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "val nombres = ds.map(_.Nombre)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6c8700-a485-4f75-bc85-a71f7d5c0bdf",
   "metadata": {},
   "source": [
    "## 3. Conversión entre DataFrames y Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866d244e-10a3-4225-840a-5d7fca1dc46e",
   "metadata": {},
   "source": [
    "**De DataFrame a Dataset**:  \n",
    "Para convertir un DataFrame a un Dataset, se necesita definir un tipo de dato (case class en Scala)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1f3ff3-c174-4d86-b0fa-1b86d3e14165",
   "metadata": {},
   "outputs": [],
   "source": [
    "case class Persona(Nombre: String, Edad: Int)\n",
    "val ds = df.as[Persona]  // DataFrame -> Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6d74f2-a871-47c9-8ee9-c51a99b16274",
   "metadata": {},
   "source": [
    "**De Dataset a DataFrame**:  \n",
    "Un Dataset puede convertirse fácilmente en un DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bea0cfa-765e-433f-a9dc-12396427ab09",
   "metadata": {},
   "outputs": [],
   "source": [
    "val df = ds.toDF()  // Dataset -> DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f7725b-3e73-4e97-9161-50f50eab8c41",
   "metadata": {},
   "source": [
    "**Ejemplo Práctico:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480d6a92-d784-47b1-8b40-6ab0a637c843",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Crear un DataFrame desde un archivo CSV\n",
    "val df = spark.read\n",
    "  .option(\"header\", \"true\")\n",
    "  .csv(\"ruta/al/archivo.csv\")\n",
    "\n",
    "// Convertir DataFrame a Dataset\n",
    "case class Persona(Nombre: String, Edad: Int)\n",
    "val ds = df.as[Persona]\n",
    "\n",
    "// Filtrar el Dataset\n",
    "val dsFiltrado = ds.filter(_.Edad > 30)\n",
    "\n",
    "// Convertir Dataset a DataFrame\n",
    "val dfFiltrado = dsFiltrado.toDF()\n",
    "\n",
    "// Mostrar resultados\n",
    "dfFiltrado.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.12.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
