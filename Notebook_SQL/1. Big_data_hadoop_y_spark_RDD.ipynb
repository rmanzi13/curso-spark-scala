{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05474fcd-c4e0-439f-8c77-08894b9ef019",
   "metadata": {},
   "source": [
    "## Big Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eca497c-a525-4fd1-9fa5-da38e75826b7",
   "metadata": {},
   "source": [
    "Big Data se refiere a las técnicas y herramientas utilizadas para procesar grandes volúmenes de datos que no pueden ser manejados por una sola computadora debido a su volumen, velocidad, variedad y veracidad (las 4 \"V\").\n",
    "\n",
    "- **Volumen:** Cantidad masiva de datos.\n",
    "\n",
    "- **Velocidad:** Rapidez con la que se generan y procesan los datos.\n",
    "\n",
    "- **Variedad:** Diferentes tipos de datos (estructurados, semi-estructurados, no estructurados).\n",
    "\n",
    "- **Veracidad:** Calidad y confiabilidad de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2532c1-f149-400c-851a-c1f7a1f038bd",
   "metadata": {},
   "source": [
    "## Ecosistema de Apache Hadoop\n",
    "\n",
    "Apache Hadoop es un framework de código abierto diseñado para el procesamiento distribuido de grandes volúmenes de datos en clústeres de computadoras. Hadoop se compone de varios módulos, entre los que destacan:\n",
    "\n",
    "- **HDFS (Hadoop Distributed File System):** Sistema de archivos distribuido que almacena datos en múltiples nodos.\n",
    "\n",
    "- **MapReduce:** Modelo de programación para procesar grandes conjuntos de datos en paralelo.\n",
    "\n",
    "- **YARN (Yet Another Resource Negotiator):** Gestor de recursos que administra los recursos del clúster y planifica las tareas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6493f5-5bc6-4671-8fae-4e86435bb063",
   "metadata": {},
   "source": [
    "![Ecosistena_Apache_Hadoop](Ecosistema_Apache_Hadoop.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247474b2-413e-4451-89c1-8eaf6e370cf0",
   "metadata": {},
   "source": [
    "#### 4. **Apache Hive**\n",
    "\n",
    "Apache Hive es una herramienta construida sobre Hadoop que permite realizar consultas SQL sobre grandes volúmenes de datos almacenados en HDFS. Hive convierte las consultas SQL en trabajos de MapReduce, lo que facilita el análisis de datos para usuarios familiarizados con SQL.\n",
    "\n",
    "**Integración de Hive con Spark**\n",
    "\n",
    "Spark puede leer y escribir datos en tablas de Hive, lo que permite combinar la potencia de Spark con la facilidad de uso de Hive para consultas SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861686de-651b-4f16-a796-041d0079047e",
   "metadata": {},
   "source": [
    "### Data Lakehouse\n",
    "Un Data Lakehouse es un repositorio centralizado que almacena grandes cantidades de datos en su formato crudo, tanto estructurados como no estructurados. Combina las ventajas de un Data Lake (almacenamiento de datos en bruto) con las de un Data Warehouse (gestión de datos estructurados)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c735285-ba1d-44f6-ba5c-88bd5757db24",
   "metadata": {},
   "source": [
    "![Data Lakehouse](datalakehouse.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16ae7ba-71b7-4930-a95d-96f27ee2bfd1",
   "metadata": {},
   "source": [
    "##  Resilient Distributed Datasets (RDDs) en Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16d893d-ccc0-4931-97b7-a4331f5ca165",
   "metadata": {},
   "source": [
    "### 1. Introducción a los RDD en Spark\n",
    "Los Resilient Distributed Datasets (RDDs) son la estructura de datos fundamental en Apache Spark. Representan una colección inmutable y distribuida de elementos que pueden procesarse en paralelo. Características clave:\n",
    "\n",
    "- **Resilientes**: Tolerantes a fallos mediante recomputación.\n",
    "\n",
    "- **Distribuidos**: Los datos se dividen en particiones y se distribuyen en el clúster.\n",
    "\n",
    "- **Inmutables**: No se modifican, pero se transforman en nuevos RDDs.\n",
    "\n",
    "- **Lazy Evaluation (Evaluación Perezosa)**:\n",
    "  \n",
    "   - Las transformaciones en RDD no se ejecutan inmediatamente\n",
    "  \n",
    "   - Spark espera hasta que se requiere una acción (como collect o count) para ejecutar las operaciones, optimizando así el procesamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06435347-b72a-440b-a831-23dbd3072387",
   "metadata": {},
   "source": [
    "### 2. Tipos de RDDs\n",
    "- **RDDs simples**: Colecciones de elementos sin esquema.\n",
    "\n",
    "- **RDDs clave-valor (Pair RDDs)**: Colecciones de pares (clave, valor).\n",
    "\n",
    "- **RDDs numéricos**: RDDs que contienen datos numéricos para operaciones estadísticas.\n",
    "\n",
    "**Ejemplos**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4c5dbc9-ab8c-4e89-ba85-1276f948956c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sc = org.apache.spark.SparkContext@55fc865d\n",
       "rddSimple = ParallelCollectionRDD[0] at parallelize at <console>:27\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at parallelize at <console>:27"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Crea un RDD con una lista de números enteros.\n",
    "val sc = spark.sparkContext\n",
    "val rddSimple = sc.parallelize(Seq(1, 2, 3, 4, 5))\n",
    "rddSimple.collect().foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0acc57b3-9b34-4e79-a191-f5be8dd950a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(A,1)\n",
      "(B,2)\n",
      "(C,3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rddPair = ParallelCollectionRDD[1] at parallelize at <console>:25\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[1] at parallelize at <console>:25"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Crea un RDD donde cada elemento es un par (clave, valor)\n",
    "val rddPair = sc.parallelize(Seq((\"A\", 1), (\"B\", 2), (\"C\", 3)))\n",
    "rddPair.collect().foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7a2fefb-5bda-4664-86a3-e2792e8aa7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Media: 25.4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rddNumerico = ParallelCollectionRDD[4] at parallelize at <console>:35\n",
       "media = 25.4\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "25.4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Se usa la extensión RDDFunctions para cálculos numéricos\n",
    "\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.mllib.rdd.RDDFunctions._\n",
    "\n",
    "val rddNumerico = sc.parallelize(Seq(10.5, 20.3, 30.7, 40.1))\n",
    "val media = rddNumerico.mean()\n",
    "println(s\"Media: $media\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a98995-a11d-440b-be56-8d71f933ed01",
   "metadata": {},
   "source": [
    "### 3. ¿Cuándo es Conveniente Usar RDDs?\n",
    "Los RDDs son útiles cuando:\n",
    "\n",
    "- Se necesita un control fino sobre las operaciones de bajo nivel.\n",
    "\n",
    "- Se trabaja con datos no estructurados o semi-estructurados.\n",
    "\n",
    "- Se requieren transformaciones complejas que no están soportadas en DataFrames o DataSets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594c592b-21a6-42e6-8bfc-010147730f9a",
   "metadata": {},
   "source": [
    "### 4. Diferencias entre DataFrames vs DataSets vs RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6355052-6013-4c17-b3ee-6e8845b7e0fc",
   "metadata": {},
   "source": [
    "| Característica   | RDDs                         | DataFrames                   | DataSets                          |\r\n",
    "|-----------------|-----------------------------|------------------------------|-----------------------------------|\r\n",
    "| Esquema        | Sin esquema                  | Con esquema                  | Con esquema (tipado fuerte)      |\r\n",
    "| Optimización   | Manual                       | Automática (Catalyst)        | Automática (Catalyst)            |\r\n",
    "| API            | Funcional (Scala, Java, Python) | SQL-like                      | Tipado fuerte (Scala, Java)      |\r\n",
    "| Rendimiento    | Menor (sin optimización)     | Mayor (optimizado)           | Mayor (optimizado y tipado)      |\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07a1888-56cb-427d-acb5-3318d7880c95",
   "metadata": {},
   "source": [
    "### 5. Creación de un RDD\n",
    "Los RDDs se pueden crear de varias formas:\n",
    "\n",
    "- **Desde una colección local**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76a123a0-16ef-4fdb-85fd-3b33aa5eb0da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd = ParallelCollectionRDD[6] at parallelize at <console>:32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[6] at parallelize at <console>:32"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(Seq(1, 2, 3, 4, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc97f5ce-d07c-4f9a-8134-54e1573d8993",
   "metadata": {},
   "source": [
    "- **Desde archivos externos**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca23ee98-a8f6-4fbf-b62f-e527c0169f72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd = ruta/al/archivo.txt MapPartitionsRDD[8] at textFile at <console>:32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "ruta/al/archivo.txt MapPartitionsRDD[8] at textFile at <console>:32"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.textFile(\"ruta/al/archivo.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987bf191-8ce7-4102-a540-221924332788",
   "metadata": {},
   "source": [
    "### 6. Interoperabilidad entre RDDs, DataSets y DataFrames\n",
    "- **De RDD a DataFrame**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09ad8be1-ccee-4c44-8e64-68fdee1ef881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|    A|\n",
      "|    B|\n",
      "|    C|\n",
      "+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rdd = ParallelCollectionRDD[18] at parallelize at <console>:43\n",
       "df = [value: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[value: string]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//toDF() funciona solo si import spark.implicits._ está presente.\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import spark.implicits._\n",
    "\n",
    "val rdd = spark.sparkContext.parallelize(Seq(\"A\", \"B\", \"C\"))\n",
    "val df = rdd.toDF(\"value\") // Se convierte en DataFrame con una columna llamada \"value\"\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc89173-f132-4d73-91cc-fe04bf2091f6",
   "metadata": {},
   "source": [
    "- **De DataFrame a RDD**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dded7ef9-c856-4655-86ef-fa778e33b89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ana,30]\n",
      "[Juan,25]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@700a548a\n",
       "df = [nombre: string, edad: int]\n",
       "rdd = MapPartitionsRDD[26] at rdd at <console>:52\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[26] at rdd at <console>:52"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession.builder.appName(\"Ejemplo\").getOrCreate()\n",
    "import spark.implicits._\n",
    "\n",
    "// Crear un DataFrame\n",
    "val df = Seq((\"Ana\", 30), (\"Juan\", 25)).toDF(\"nombre\", \"edad\")\n",
    "\n",
    "// Convertir DataFrame a RDD\n",
    "val rdd = df.rdd\n",
    "\n",
    "// Imprimir el contenido del RDD\n",
    "rdd.collect().foreach(println)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf99b90f-a985-4054-9866-873583a8a427",
   "metadata": {},
   "source": [
    "- **De RDD a DataSet**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "786cca51-3770-4320-9660-13cc235a935f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "|nombre|edad|\n",
      "+------+----+\n",
      "|   Ana|  30|\n",
      "|  Juan|  25|\n",
      "+------+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@700a548a\n",
       "defined class Persona\n",
       "rdd = ParallelCollectionRDD[14] at parallelize at <console>:41\n",
       "ds = [nombre: string, edad: int]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[nombre: string, edad: int]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "val spark = SparkSession.builder.appName(\"Ejemplo\").getOrCreate()\n",
    "import spark.implicits._\n",
    "\n",
    "// Crear un RDD de una case class\n",
    "case class Persona(nombre: String, edad: Int)\n",
    "val rdd = spark.sparkContext.parallelize(Seq(Persona(\"Ana\", 30), Persona(\"Juan\", 25)))\n",
    "\n",
    "// Convertir RDD a Dataset\n",
    "val ds = rdd.toDS()\n",
    "ds.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ab98faf-fbba-473d-839e-5163368414a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Ana,30)\n",
      "(Juan,25)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rddTipado = MapPartitionsRDD[27] at map at <console>:47\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[27] at map at <console>:47"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rddTipado = df.rdd.map(row => (row.getString(0), row.getInt(1)))\n",
    "rddTipado.collect().foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c44eb50-d69a-4a4b-b9db-37b4ea5cbed6",
   "metadata": {},
   "source": [
    "### 7. Creación de un RDD a partir de una Colección Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1387f046-f925-454f-817a-3f5904d611d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "datos = List(1, 2, 3, 4, 5)\n",
       "rdd = ParallelCollectionRDD[30] at parallelize at <console>:50\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[30] at parallelize at <console>:50"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val datos = Seq(1, 2, 3, 4, 5)\n",
    "val rdd = sc.parallelize(datos)\n",
    "\n",
    "// Imprimir el contenido del RDD\n",
    "rdd.collect().foreach(println) //collect(): Recoge todos los elementos del RDD y los devuelve como un array en el driver.\n",
    "                               //foreach(println): Itera sobre los elementos del array y los imprime."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0dce3c-b3f6-44f0-9e10-38061ebaccc5",
   "metadata": {},
   "source": [
    "### 8. Creación de un RDD de Distintos Orígenes\n",
    "- **Desde un archivo de texto**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a9a578-dd20-4826-ad55-da6f4cf12e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val rdd = sc.textFile(\"ruta/al/archivo.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71484596-0f7a-4cf6-9077-e8524e327a8e",
   "metadata": {},
   "source": [
    "- **Desde HDFS**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a517fc2f-1f73-4820-9383-1ebf7d4c1efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "val rdd = sc.textFile(\"hdfs://ruta/al/archivo.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70671169-2991-4fc0-950a-98c0a44e3f83",
   "metadata": {},
   "source": [
    "- **Desde una base de datos**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c794cae-a726-4b07-a5e2-e4166a35c9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "val rdd = sc.jdbcRDD(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4322e6-a793-4833-a733-cff0707819d2",
   "metadata": {},
   "source": [
    "### 9. Manipulación de RDDs\n",
    "**a) Transformaciones**\n",
    "Las transformaciones crean un nuevo RDD a partir de uno existente:\n",
    "\n",
    "- **map**: Aplica una función a cada elemento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e927183b-fe9b-4f8c-aab5-39bc0315f6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "val rdd2 = rdd.map(x => x * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e5815b-8ff1-464c-882e-16869c6484df",
   "metadata": {},
   "source": [
    "- **filter**: Filtra elementos según una condición."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba20a3c-b267-4634-a39c-1a7b8e444b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "val rdd2 = rdd.filter(x => x > 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dc9228-ee71-432c-b22c-9164f58ebd77",
   "metadata": {},
   "source": [
    "- **flatMap**: Aplica una función que devuelve múltiples elementos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c1c230-8431-4157-94d3-d8754ed4f6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "val rdd2 = rdd.flatMap(x => Seq(x, x * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02352722-5a54-42b0-840b-18c4f966e28f",
   "metadata": {},
   "source": [
    "**b) Acciones**\n",
    "Las acciones devuelven un valor o escriben datos:\n",
    "\n",
    "- **count**: Cuenta el número de elementos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a576528-9181-4450-892f-231c139a9c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val total = rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903e7cac-4825-4b4a-9887-65cfcdfef384",
   "metadata": {},
   "source": [
    "- **collect**: Recupera todos los elementos al driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bc2cb4-5504-4aa5-ab47-602ac6080e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "val datos = rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbfdec2-6417-47aa-b24d-34de57a30d13",
   "metadata": {},
   "source": [
    "- **reduce**: Combina elementos usando una función."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2258873e-d4bd-469c-b5af-eafc9b3b6924",
   "metadata": {},
   "outputs": [],
   "source": [
    "val suma = rdd.reduce((x, y) => x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7f42fa-f200-4f72-8298-39d074cc9b62",
   "metadata": {},
   "source": [
    "### 10. Guardado de Archivos\n",
    "- **Archivos de texto**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c8bb03-1252-4c4e-b1d6-f12510d5123f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.saveAsTextFile(\"ruta/de/salida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce71724-d09b-4a12-83ab-2f92ac6852fb",
   "metadata": {},
   "source": [
    "- **Archivos Hadoop**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647dcb42-c27e-40c3-95fe-154bf189189f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.saveAsHadoopFile(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dca866-cff1-4a61-9d96-b64f4333e1d3",
   "metadata": {},
   "source": [
    "- **Otros formato**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305e2d28-3b99-4a69-8683-08c23a3231d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.saveAsObjectFile(\"ruta/de/salida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c615076-e33d-4a0e-beb7-1a0e945aeb91",
   "metadata": {},
   "source": [
    "### 11. Cacheo de RDDs\n",
    "El cacheo permite almacenar RDDs en memoria o disco para reutilizarlos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94eb5297-c21a-461e-8a56-f2658719a379",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.cache()  // Almacena en memoria\n",
    "rdd.persist(StorageLevel.DISK_ONLY)  // Almacena en disco"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062bbaeb-1d5f-4c5b-bba0-051b40619bf8",
   "metadata": {},
   "source": [
    "### 12. Establecimiento de Puntos de Control\n",
    "Los puntos de control (checkpoints) permiten guardar el estado de un RDD en un almacenamiento confiable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6d7ce7-8828-413e-9f4d-2d28742d425a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setCheckpointDir(\"ruta/de/checkpoint\")\n",
    "rdd.checkpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe29a22-d259-4827-a566-84cd56949c5f",
   "metadata": {},
   "source": [
    "#### **Conclusión**\n",
    "\n",
    "Los RDDs son la base de Apache Spark y ofrecen un control fino sobre el procesamiento distribuido. Aunque DataFrames y DataSets son más modernos y optimizados, los RDDs siguen siendo útiles para casos específicos que requieren operaciones de bajo nivel. Con un manejo adecuado de transformaciones, acciones y técnicas como el cacheo, se pueden construir aplicaciones eficientes y escalables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193bbe01-e625-488c-bb69-e28c78fe6d27",
   "metadata": {},
   "source": [
    "## **Apache Spark**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530619c9-efe0-477a-b468-7efe45810d25",
   "metadata": {},
   "source": [
    "#### **1. ¿Qué es Apache Spark?**\n",
    "\n",
    "Apache Spark es un motor de procesamiento de datos distribuido y en memoria, diseñado para ser rápido y eficiente en el manejo de grandes volúmenes de datos. A diferencia de Hadoop MapReduce, que escribe y lee datos del disco en cada paso, Spark mantiene los datos en memoria, lo que lo hace hasta 100 veces más rápido para ciertas cargas de trabajo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5dbf69-8bd4-40be-8fb0-f1f312e5e712",
   "metadata": {},
   "source": [
    "#### **Ventajas de Spark sobre Hadoop MapReduce:**\n",
    "\n",
    "- **Velocidad:** Al procesar datos en memoria, Spark evita la sobrecarga de E/S (entrada/salida) en disco, lo que lo hace ideal para aplicaciones que requieren baja latencia.\n",
    "\n",
    "- **Facilidad de uso:** Spark ofrece APIs en Scala, Java, Python y R, lo que lo hace accesible para una amplia gama de desarrolladores.\n",
    "\n",
    "- **Unificación:** Spark integra múltiples componentes (SQL, streaming, machine learning, procesamiento de grafos) en una sola plataforma, eliminando la necesidad de usar herramientas separadas.\n",
    "\n",
    "- **Tolerancia a fallos:** Spark utiliza RDD (Resilient Distributed Datasets), que son colecciones de datos distribuidos y tolerantes a fallos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbe30b1-fbdd-4028-8c77-ade83f4d7fca",
   "metadata": {},
   "source": [
    "#### **Casos de uso en ciencia de datos:**\n",
    "\n",
    "- **Procesamiento de datos a gran escala:** Limpieza, transformación y análisis de grandes volúmenes de datos.\n",
    "\n",
    "- **Machine Learning:** Entrenamiento de modelos con grandes conjuntos de datos usando MLlib.\n",
    "\n",
    "- **Streaming en tiempo real:** Procesamiento de flujos de datos en tiempo real con Spark Streaming.\n",
    "\n",
    "- **Análisis de grafos:** Procesamiento de datos en forma de grafos con GraphX."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fe1ca4-a656-4c9b-be55-97173c224c34",
   "metadata": {},
   "source": [
    "#### **2. Componentes principales de Spark**\n",
    "\n",
    "Spark es una plataforma modular que incluye varios componentes para diferentes tipos de procesamiento de datos:\n",
    "\n",
    "**1. Spark Core:**\n",
    "\n",
    "- Es el núcleo de Spark y proporciona la funcionalidad básica para la ejecución distribuida de tareas.\n",
    "\n",
    "- Incluye soporte para RDD, que son la base de todas las operaciones en Spark.\n",
    "\n",
    "**2. Spark SQL:**\n",
    "\n",
    "- Permite trabajar con datos estructurados usando consultas SQL.\n",
    "\n",
    "- Proporciona una interfaz unificada para trabajar con DataFrames y Datasets.\n",
    "\n",
    "- Ejemplo: Consultas SQL sobre grandes volúmenes de datos almacenados en HDFS o bases de datos.\n",
    "\n",
    "**3. Spark Streaming:**\n",
    "\n",
    "- Permite procesar flujos de datos en tiempo real.\n",
    "\n",
    "- Ejemplo: Análisis de logs en tiempo real o procesamiento de datos de sensores.\n",
    "\n",
    "**4. MLlib (Machine Learning Library):**\n",
    "\n",
    "- Biblioteca de machine learning escalable que incluye algoritmos como clasificación, regresión, clustering y recomendación.\n",
    "\n",
    "- Ejemplo: Entrenamiento de modelos predictivos con grandes conjuntos de datos.\n",
    "\n",
    "**5. GraphX:**\n",
    "\n",
    "- Biblioteca para el procesamiento de grafos y análisis de redes.\n",
    "\n",
    "- Ejemplo: Análisis de redes sociales o detección de comunidades en grafos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.12.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
